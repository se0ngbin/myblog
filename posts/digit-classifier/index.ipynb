{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf354ea",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"MNIST Digit Classifier\"\n",
    "author: \"Seongbin Park\"\n",
    "jupyter: \"python3\"\n",
    "categories: [ml, projects, kaggle-competition]\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "page-layout: article\n",
    "description: How to build a (very) simple neural network for MNIST digit classification\n",
    "date: \"2022-08-05\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7dcb3c",
   "metadata": {},
   "source": [
    "This post will cover how to classify handwritten digits of the MNIST dataset using a simple neural network. At the same time, I will be taking a stab at the [Kaggle Digit Recognizer](https://www.kaggle.com/competitions/digit-recognizer/overview) contest.\n",
    "\n",
    "Credits: I will be working off of chapter 4 of the [fast.ai](https://github.com/fastai/fastbook) book, which covers binary classification of 3's and 7's. Other resources are linked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb0be789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |code-fold: true\n",
    "# |code-summary: required libraries\n",
    "\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f668777",
   "metadata": {},
   "source": [
    "## Dowloading the Data\n",
    "First, we will have to import the MNIST dataset itself. We can import it using the fast.ai library (`path = untar_data(URLs.MNIST)`), but I will download the dataset from [kaggle](https://www.kaggle.com/competitions/digit-recognizer/) instead.\n",
    "\n",
    "If you are following along and haven't set up the kaggle API yet, do so by following along the README of the official [repo](https://github.com/Kaggle/kaggle-api). You will need an account to do so. After everything is set up, we can run the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87936ceb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading digit-recognizer.zip to /home/jupyter/projects/digit-classifier\n",
      "  0%|                                               | 0.00/15.3M [00:00<?, ?B/s]\n",
      "100%|███████████████████████████████████████| 15.3M/15.3M [00:00<00:00, 161MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c digit-recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beff17f3",
   "metadata": {},
   "source": [
    "Note that in Jupyter notebooks, the exclamation mark ! is used to execute shell commands. The dataset should be downloaded in your project directory as a zip file. Run the following code block to extract the contents to a file named MNIST_dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51779ab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  digit-recognizer.zip\n",
      "  inflating: MNIST_dataset/sample_submission.csv  \n",
      "  inflating: MNIST_dataset/test.csv  \n",
      "  inflating: MNIST_dataset/train.csv  \n"
     ]
    }
   ],
   "source": [
    "!unzip digit-recognizer.zip -d MNIST_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c169646",
   "metadata": {},
   "source": [
    "Let's take a look at `test.csv` (the test set) and `train.csv` (the training set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a9f11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = Path(\"./MNIST_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91967166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "\n",
       "[3 rows x 784 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test.csv\n",
    "df_test = pd.read_csv(ds_path/\"test.csv\")\n",
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "539f2a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "\n",
       "[3 rows x 785 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train.csv\n",
    "df_train = pd.read_csv(ds_path/\"train.csv\")\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500c721",
   "metadata": {},
   "source": [
    "Now that we downloaded the data, we need to shape it for training and validating.\n",
    "\n",
    "## Shaping the Data\n",
    "\n",
    "To train our model, we need to separate and normalize the independent (pixels) and dependent (label) variables. The labels will be represented using [one hot encoding](https://en.wikipedia.org/wiki/One-hot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "329e3566",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tensor(df_train.drop(labels = ['label'],axis = 1)) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "568f8696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([42000, 784]), torch.Size([42000, 10]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_numeric = df_train['label']\n",
    "\n",
    "rows = np.arange(y_train_numeric.size)\n",
    "y_train = tensor(np.zeros((y_train_numeric.size, 10)))\n",
    "y_train[rows, y_train_numeric] = 1\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ad8c22",
   "metadata": {},
   "source": [
    "`X_train.shape` and `y_train.shape` tells us that we have 42000 digits in our dataset, with each digit having 784 pixels. We will use tensors to take advantage of faster GPU computations.\n",
    "\n",
    "We want to create a Pytorch [`Dataset`](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html), which is required to return a tuple of `(x,y)` when indexed. Python provides a [`zip`](https://www.w3schools.com/python/ref_func_zip.asp) function which, when combined with [`list`](https://www.w3schools.com/python/ref_func_list.asp), can do this easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a037318",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7373, 1.0000, 0.3686,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7490, 0.9804, 0.9922,\n",
       "         0.3647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4824, 0.9725, 0.9922,\n",
       "         0.6549, 0.0392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3137, 0.9686, 0.9922,\n",
       "         0.8157, 0.0510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1137, 0.8118, 0.9922,\n",
       "         0.9216, 0.3020, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2118, 0.8196, 0.9922,\n",
       "         0.9922, 0.3451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3647, 0.9961, 0.9922,\n",
       "         0.9333, 0.6667, 0.0667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0902, 0.8235, 0.9961,\n",
       "         0.9922, 0.6235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627, 0.8196, 0.9922,\n",
       "         0.9961, 0.9412, 0.3176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1059, 0.9922,\n",
       "         0.9922, 0.9961, 0.0510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0784, 0.8078,\n",
       "         0.9961, 0.9961, 0.7765, 0.0275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6588,\n",
       "         0.9922, 0.9922, 0.7686, 0.0275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0784,\n",
       "         0.7961, 0.9922, 0.9725, 0.2980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0863,\n",
       "         0.7373, 0.9922, 0.9608, 0.3647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.4039, 0.9922, 0.9922, 0.7490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.3490, 0.9412, 0.9922, 0.7647, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0588, 0.8627, 0.9922, 0.9922, 0.3137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.3686, 0.9922, 0.9922, 0.9922, 0.3686, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.3490, 0.9843, 0.9922, 0.9804, 0.5137, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.8392, 0.8549, 0.3725, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000]),\n",
       " tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |output: false\n",
    "\n",
    "ds = list(zip(X_train,y_train))\n",
    "ds[0]\n",
    "# output removed for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df78bad9",
   "metadata": {},
   "source": [
    "Next, we want to split our dataset `ds` into a training and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21ea2517",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = torch.utils.data.random_split(ds,[32000, 10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d15c7cf",
   "metadata": {},
   "source": [
    "Later, we will be using stochastic gradient descent, which requires that we have \"mini-batches\" of our dataset. We can create a `DataLoader` from our `train` dataset to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daf9da63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 10]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(train, batch_size=256)\n",
    "xb,yb = first(dl)\n",
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3372ca05",
   "metadata": {},
   "source": [
    "We can do the same for our validation (`val`) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "606d0bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl = DataLoader(val, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d175742",
   "metadata": {},
   "source": [
    "## Training a Linear Model\n",
    "\n",
    "Now that our data is ready, we can start training our classification model. We will start with a linear model, then add some non-linearity to it!\n",
    "\n",
    "First, we must randomly initialize the bias and all weights for each pixel. Since we have 10 labels (one for each digit), there must be 10 outputs, so our weights matrix is of size `784x10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b84c8398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n",
    "\n",
    "weights = init_params((784,10))\n",
    "bias = init_params(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cbef47",
   "metadata": {},
   "source": [
    "The prediction given a tensor `x` is\n",
    "\n",
    "$$\\text{prediction} = x \\cdot \\text{weights} + \\text{bias}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d5b69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear1(xb): return xb@weights + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aaeb28",
   "metadata": {},
   "source": [
    "To calculate a gradient, we need a loss function. Since there are more than 2 labels, we will use [cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), which is related to the [softmax](https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax) function instead of a sigmoid function (which is used for binary classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1600c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(xb, yb):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    return loss(xb, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b23adda",
   "metadata": {},
   "source": [
    "For testing and demonstration purposes, let's work with a smaller batch than the ones created when shaping our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "153245e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 784])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = X_train[:4]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cdcfdf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.5287,  12.6047,  -9.0290,  -1.7505,   3.7686,  18.8489,  -1.8141,\n",
       "         -12.2232,  -5.1421,  -6.6316],\n",
       "        [ 18.8942,  10.7898,   9.2573,   7.9989,  -1.2884,  19.0238,  -5.8788,\n",
       "           6.5045, -10.2431,   5.5865],\n",
       "        [  6.3639,  14.0687,   0.7705,  -1.3580,   1.4220,   7.3108,  -7.4359,\n",
       "          -6.8101,  -5.9212,  23.7016],\n",
       "        [-14.7847,   3.0711,  -0.6092,   2.2720,  -1.1361,   3.7617,   5.1197,\n",
       "           5.3868,  -1.5228,  -7.6523]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = linear1(batch)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c8f780d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.9773, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mnist_loss(preds, y_train[:4])\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8edecb7",
   "metadata": {},
   "source": [
    "Now we can calculate the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19f7d5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 10]), tensor(2.4328e-10), tensor([5.9605e-08]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "weights.grad.shape,weights.grad.mean(),bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f498aa7b",
   "metadata": {},
   "source": [
    "The following function combines the above code and generalizes to models other than `linear1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc90a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58a41c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.8657e-10), tensor([1.1921e-07]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_grad(batch, y_train[:4], linear1)\n",
    "weights.grad.mean(),bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b987911",
   "metadata": {},
   "source": [
    "Using the calculated gradients, we can update the weights for each epoch. We need to specify a learning rate and reset the gradients to 0, since `loss.backward` actually adds the gradients of loss to any gradients that are currently stored.\n",
    "```Python\n",
    "lr = 1.\n",
    "\n",
    "for p in weights,bias:\n",
    "    p.data -= p.grad*lr\n",
    "    p.grad.zero_()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a17a65",
   "metadata": {},
   "source": [
    "Finally, we can define a function that trains the model for one epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76363e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, params, lr=1):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8f980d",
   "metadata": {},
   "source": [
    "We also probably want to check the accuracy of our model. The label that the model predicts is the label with the highest activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53401855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(xb, yb):\n",
    "    label = torch.argmax(xb, dim=1)\n",
    "    y_truth = torch.argmax(yb, dim=1)\n",
    "    correct = y_truth == label\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89932128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_accuracy(linear1(batch), y_train[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324a07b8",
   "metadata": {},
   "source": [
    "To get the accuracy for the whole epoch, we must call `batch_accuracy` with batches of the validation dataset, then take the mean over all batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7767ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f951ca",
   "metadata": {},
   "source": [
    "Finally, we can see if our code works by checking if the accuracy improves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f188004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8213 0.8532 0.8661 0.8736 0.8769 0.8808 0.8848 0.8869 0.8895 0.8914 0.8929 0.894 0.8949 0.8964 0.8973 0.8977 0.8978 0.8979 0.8981 0.8986 0.8998 0.9 0.9004 0.9016 0.9017 0.9027 0.9027 0.9032 0.9037 0.9047 0.9053 0.9058 0.9061 0.9062 0.9061 0.9062 0.906 0.9061 0.906 0.9063 "
     ]
    }
   ],
   "source": [
    "params = weights, bias\n",
    "for i in range(40):\n",
    "    train_epoch(linear1, params)\n",
    "    print(validate_epoch(linear1), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b7d7f6",
   "metadata": {},
   "source": [
    "## Simplifying Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639e28e6",
   "metadata": {},
   "source": [
    "`nn.Linear` does the same thing as our `init_params` and `linear1` functions together. Also, fastai's `SGD` class provides us with functions that takes care of updating the parameters and reseting the gradients of our model. By replacing some code, we can boil the training portion of our MNIST classifer down to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dac73f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8983 0.9057 0.9092 0.9113 0.9143 0.9149 0.9154 0.9154 0.9162 0.9161 0.9166 0.9167 0.9166 0.9166 0.9164 0.9169 0.917 0.9173 0.9173 0.9175 "
     ]
    }
   ],
   "source": [
    "def mnist_loss(xb, yb):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    return loss(xb, yb)\n",
    "\n",
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    loss.backward()\n",
    "\n",
    "def train_epoch_simple(model):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "def train_model(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch_simple(model)\n",
    "        print(validate_epoch(model), end=' ')\n",
    "        \n",
    "linear_model = nn.Linear(28*28,10)\n",
    "opt = SGD(linear_model.parameters(), lr=1)\n",
    "train_model(linear_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38da29e4",
   "metadata": {},
   "source": [
    "Fast.ai provides us with `Learner.fit`, which we can use instead of `train_model` to significantly reduce the amount of code we need to write. To use the function, we must create a `Learner`, which requires a `DataLoaders` of our training and validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e870870",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6112f9a3",
   "metadata": {},
   "source": [
    "Then, we pass in `DataLoaders`, the model, the optimization function, the loss function, and optionally any metrics to print into the `Learner` constructor to create one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5f57522",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, nn.Linear(28*28,10), opt_func=SGD,\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0754164b",
   "metadata": {},
   "source": [
    "Finally, we can call `Learner.fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ef06e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.413918</td>\n",
       "      <td>0.365917</td>\n",
       "      <td>0.897400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.326721</td>\n",
       "      <td>0.337396</td>\n",
       "      <td>0.905100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.302027</td>\n",
       "      <td>0.325679</td>\n",
       "      <td>0.908500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.289524</td>\n",
       "      <td>0.319092</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.281248</td>\n",
       "      <td>0.314855</td>\n",
       "      <td>0.912800</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.275084</td>\n",
       "      <td>0.311911</td>\n",
       "      <td>0.913200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.270192</td>\n",
       "      <td>0.309764</td>\n",
       "      <td>0.913500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.266153</td>\n",
       "      <td>0.308146</td>\n",
       "      <td>0.913900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.262723</td>\n",
       "      <td>0.306901</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.259748</td>\n",
       "      <td>0.305928</td>\n",
       "      <td>0.914900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(10, lr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d85b3",
   "metadata": {},
   "source": [
    "## Adding Non-linearity\n",
    "\n",
    "To expand upon our model, we can add another layer on top of what we have now. However, mathematically speaking, the composition of two linear functions is another linear function. Therefore, stacking two linear classifiers on top of each other is equivalent to having just one linear classifier.\n",
    "\n",
    "Therefore, we must add some non-linearity between linear layers. We often do this by through activation functions; a common one is the [`ReLU`](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30f3fe39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4221aaf390>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-5,5,100)\n",
    "y = np.maximum(0, x)\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9f1e4",
   "metadata": {},
   "source": [
    "[`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) creates a module that will call each of the listed layers or functions.\n",
    "\n",
    "Our first layer takes in 784 inputs (pixels) and outputs 60 numbers. Those 60 numbers are then each passed into the `ReLU` function before going into the second layer. The second layer has 10 outputs, which as before, is the probability of each digit being the lable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7f0ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(784,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5484633",
   "metadata": {},
   "source": [
    "We can train this model using `Learner.fit` as well (we are using more epochs and smaller learning rate, since it is a larger model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b29a7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.781461</td>\n",
       "      <td>0.532242</td>\n",
       "      <td>0.866300</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.441000</td>\n",
       "      <td>0.395819</td>\n",
       "      <td>0.893000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.355962</td>\n",
       "      <td>0.352042</td>\n",
       "      <td>0.902600</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.319263</td>\n",
       "      <td>0.327102</td>\n",
       "      <td>0.909500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.295532</td>\n",
       "      <td>0.308874</td>\n",
       "      <td>0.913600</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.276869</td>\n",
       "      <td>0.293574</td>\n",
       "      <td>0.917000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.261044</td>\n",
       "      <td>0.280591</td>\n",
       "      <td>0.920400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.247288</td>\n",
       "      <td>0.268860</td>\n",
       "      <td>0.924300</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.235076</td>\n",
       "      <td>0.258463</td>\n",
       "      <td>0.927900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.223990</td>\n",
       "      <td>0.248834</td>\n",
       "      <td>0.930800</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.213890</td>\n",
       "      <td>0.240252</td>\n",
       "      <td>0.932900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.204643</td>\n",
       "      <td>0.232565</td>\n",
       "      <td>0.935200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.196180</td>\n",
       "      <td>0.225423</td>\n",
       "      <td>0.937000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.188346</td>\n",
       "      <td>0.218758</td>\n",
       "      <td>0.939200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.181094</td>\n",
       "      <td>0.212594</td>\n",
       "      <td>0.939900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.174359</td>\n",
       "      <td>0.206947</td>\n",
       "      <td>0.940100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.168056</td>\n",
       "      <td>0.201667</td>\n",
       "      <td>0.941700</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.162067</td>\n",
       "      <td>0.196636</td>\n",
       "      <td>0.943100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.156417</td>\n",
       "      <td>0.191967</td>\n",
       "      <td>0.944300</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.151026</td>\n",
       "      <td>0.187445</td>\n",
       "      <td>0.945500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.145904</td>\n",
       "      <td>0.183263</td>\n",
       "      <td>0.946400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.179306</td>\n",
       "      <td>0.947400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.136399</td>\n",
       "      <td>0.175533</td>\n",
       "      <td>0.948500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.132011</td>\n",
       "      <td>0.171847</td>\n",
       "      <td>0.949300</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.127819</td>\n",
       "      <td>0.168481</td>\n",
       "      <td>0.950700</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.123817</td>\n",
       "      <td>0.165292</td>\n",
       "      <td>0.951900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.120019</td>\n",
       "      <td>0.162331</td>\n",
       "      <td>0.952500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.116390</td>\n",
       "      <td>0.159504</td>\n",
       "      <td>0.953500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.112947</td>\n",
       "      <td>0.156784</td>\n",
       "      <td>0.954100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.109665</td>\n",
       "      <td>0.154226</td>\n",
       "      <td>0.955200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.106508</td>\n",
       "      <td>0.151784</td>\n",
       "      <td>0.955900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.103485</td>\n",
       "      <td>0.149547</td>\n",
       "      <td>0.956500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.100609</td>\n",
       "      <td>0.147393</td>\n",
       "      <td>0.957300</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.097871</td>\n",
       "      <td>0.145380</td>\n",
       "      <td>0.957700</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.095229</td>\n",
       "      <td>0.143457</td>\n",
       "      <td>0.958700</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.092694</td>\n",
       "      <td>0.141595</td>\n",
       "      <td>0.959100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.090273</td>\n",
       "      <td>0.139827</td>\n",
       "      <td>0.959100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.087953</td>\n",
       "      <td>0.138202</td>\n",
       "      <td>0.959800</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.085736</td>\n",
       "      <td>0.136662</td>\n",
       "      <td>0.960100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.083610</td>\n",
       "      <td>0.135193</td>\n",
       "      <td>0.960400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.081549</td>\n",
       "      <td>0.133738</td>\n",
       "      <td>0.960800</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.079560</td>\n",
       "      <td>0.132337</td>\n",
       "      <td>0.961600</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.077661</td>\n",
       "      <td>0.131035</td>\n",
       "      <td>0.962100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.075826</td>\n",
       "      <td>0.129802</td>\n",
       "      <td>0.962100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.074042</td>\n",
       "      <td>0.128692</td>\n",
       "      <td>0.962200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.072344</td>\n",
       "      <td>0.127533</td>\n",
       "      <td>0.962300</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.070702</td>\n",
       "      <td>0.126541</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.069096</td>\n",
       "      <td>0.125475</td>\n",
       "      <td>0.963100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.067572</td>\n",
       "      <td>0.124556</td>\n",
       "      <td>0.963300</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.066088</td>\n",
       "      <td>0.123669</td>\n",
       "      <td>0.963600</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.064652</td>\n",
       "      <td>0.122781</td>\n",
       "      <td>0.963900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.063260</td>\n",
       "      <td>0.121974</td>\n",
       "      <td>0.964200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.061913</td>\n",
       "      <td>0.121199</td>\n",
       "      <td>0.964300</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.060608</td>\n",
       "      <td>0.120421</td>\n",
       "      <td>0.964600</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.059330</td>\n",
       "      <td>0.119673</td>\n",
       "      <td>0.964700</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.058102</td>\n",
       "      <td>0.118998</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.056910</td>\n",
       "      <td>0.118366</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.055744</td>\n",
       "      <td>0.117742</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.054619</td>\n",
       "      <td>0.117166</td>\n",
       "      <td>0.965100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.053524</td>\n",
       "      <td>0.116611</td>\n",
       "      <td>0.965200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# |output: false\n",
    "learn = Learner(dls, simple_net, opt_func=SGD,\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)\n",
    "learn.fit(60, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f27fa1",
   "metadata": {},
   "source": [
    "The output is ommitted to save room; the training process is recorded in `learn.recorder`, with the table of output stored in the `values` attribute, so we can plot the accuracy over training as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b507f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4221be7610>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(L(learn.recorder.values).itemgot(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22504823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9652000069618225"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final accuracy\n",
    "learn.recorder.values[-1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa793fb",
   "metadata": {},
   "source": [
    "## Making a Submission\n",
    "\n",
    "Though our very basic model is far from perfect, we can still submit it to the competition! Recall that we stored the test.csv data into the df_test `DataFrame`. We need to first normalize the pixels then plug it into our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f36a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tensor(df_test)/ 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9dd447c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_val = simple_net(X_test)\n",
    "y_pred = torch.argmax(x_val, dim=1)\n",
    "y_pred = y_pred.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba18a48e",
   "metadata": {},
   "source": [
    "Finally, we can create a submission file in our current directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "407ff62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission.csv\", 'w+') as f :\n",
    "    f.write('ImageId,Label\\n')\n",
    "    for i in range(len(y_pred)) :\n",
    "        f.write(\"\".join([str(i+1),',',str(y_pred[i]),'\\n']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba5eca",
   "metadata": {},
   "source": [
    "Then submit to Kaggle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e4ae00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 208k/208k [00:01<00:00, 165kB/s]\n",
      "Successfully submitted to Digit Recognizer"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c digit-recognizer -f submission.csv -m \"First Attempt\""
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
