<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Seongbin Park">
<meta name="dcterms.date" content="2022-10-15">
<meta name="description" content="How and why gradient descent works">

<title>seong/bin/blog - Gradient Descent Explained</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">seong/bin/blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/se0ngbin"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/seongbeanie"><i class="bi bi-linkedin" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Gradient Descent Explained</h1>
                  <div>
        <div class="description">
          How and why gradient descent works
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">ml</div>
                <div class="quarto-category">notes</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Seongbin Park </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 15, 2022</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Gradient Descent is a methodology to tune parameters to minimize loss functions for machine learning. If you don’t know what a loss function is, you can read my article on loss functions here.</p>
<p>This article will how and why gradient descent works; to follow along, you should know what a gradient is (duh). If you don’t, give this article a read.</p>
<section id="how-gradient-descent-works" class="level1">
<h1>How gradient descent works</h1>
<p>Gradient descent can be summarized into the following steps:</p>
<ol type="1">
<li>Choose initial values of parameters <span class="math inline">\((\theta = \{\theta_0, \theta_1, \dots, \theta_d\} \in \mathbb{R}^{d+1})\)</span></li>
<li>Step into the opposite direction of the gradient of the loss function by a factor of the learning rate: <span class="math display">\[\theta_j \leftarrow\theta_j-\alpha \frac{\partial J(\theta)}{\partial\theta_j}\qquad \forall j =0\dots d \quad \text{simultaneously}\]</span></li>
<li>Repeat unitl convergence (when the Euclidian norm between updated parameters converges: <span class="math inline">\(\|\theta_\text{new}-\theta_\text{old}\|&lt;\epsilon\)</span> for some <span class="math inline">\(\epsilon\)</span>)</li>
</ol>
<p>A lot of notation; kinda confusing. In human language, here is what happens:</p>
<ol type="1">
<li>Start off with a set of parameters.</li>
<li>Repeat the following steps until the parameters don’t change much:
<ol type="a">
<li>Calculate the gradient of the loss function with respect to each parameter</li>
<li>Move the parameters in the opposite direction of the gradient by a factor of the learning rate</li>
</ol></li>
</ol>
<blockquote class="blockquote">
<p>Note: The learning rate <span class="math inline">\(\alpha\)</span> is a hyperparameter that controls how big the steps are. I will cover it in following sections, but to learn about hyperparameters in general, click here.</p>
</blockquote>
<p>It almost seems magical that this simple procedure can find the minimum of a loss function. To figure out why this works, we need to figure out what exactly moving in the opposite direction of the gradient means.</p>
</section>
<section id="why-gradient-descent-works" class="level1">
<h1>Why gradient descent works</h1>
<p>Imagine you are a hiker on a mountain that wants to find the lowest point. To get to the lowest point as quickly as possible, you probably want to walk in the direction of steepest descent.</p>
<p>However, since a mountain is irregular, the direction of steepest descent is not always the same. Therefore, you will have to take a small step, recalculate the direction, take another small step, and so on. Eventually, you will find the lowest point.</p>
<p><img src="gd.png" title="credit: https://medium.com/@DBCerigo/on-why-gradient-descent-is-even-needed-25160197a635" class="img-fluid"></p>
<p>This is exactly what gradient descent does. It takes small steps in the direction of steepest descent until the cost function is minimized.</p>
<p>In other words, moving in the opposite direction of the gradient is equivalent to taking a step in the direction of steepest descent. This can be proven if we prove that the gradient is the direction of steepest ascent. In the following section, I will present a proof that works for any dimension; feel free to skip it if you are not interested.</p>
<section id="proof-the-gradient-is-the-direction-of-steepest-descent" class="level3">
<h3 class="anchored" data-anchor-id="proof-the-gradient-is-the-direction-of-steepest-descent">Proof: The gradient is the direction of steepest descent</h3>
<p>Suppose <span class="math inline">\(f(x,y)\)</span> is differentiable at <span class="math inline">\((x,y)\)</span>, <span class="math inline">\(\nabla f(x,y) \ne \vec{0}\)</span>, and <span class="math inline">\(\vec{u}\)</span> is a unit vector. The directional derivative of <span class="math inline">\(f\)</span> in the direction of <span class="math inline">\(\vec{u}\)</span> can be calculated as follows: <span class="math display">\[{D_{\vec u}}f\left( {x,y} \right) = \nabla f \cdot \vec{u}\]</span></p>
<p>We can decompose the dot product into the following: <span class="math display">\[\begin{align}
{D_{\vec u}}f\left( {x,y} \right) &amp;= \nabla f \cdot \vec{u} \\ &amp;= \|\nabla f\|\|\vec{u}\|\cos{\theta} \\ &amp;= \|\nabla f\|\cos{\theta}.
\end{align}\]</span> Since <span class="math inline">\(-1 \le \cos{\theta} \le 1\)</span>, the directional derivative, or the rate of change of <span class="math inline">\(f\)</span>, is maximum when <span class="math inline">\(\theta = 0\)</span>, which is when it points in the same direction of <span class="math inline">\(\nabla f. \qquad \square\)</span></p>
</section>
</section>
<section id="learning-rate" class="level1">
<h1>Learning rate</h1>
<p>As mentioned earlier, <span class="math inline">\(\alpha\)</span> is the learning rate, a hyperparameter that controls how big the steps are. To see why it matters, let’s go back to the hiking analogy.</p>
<p>If the hiker takes too many steps in one direction before recalculating the gradient, he or she might continuously overshoot the lowest point. If the hiker takes too few steps, he or she might never reach the lowest point in time.</p>
<p><img src="https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png" class="img-fluid"></p>
<p>Likewise, if the learning rate is too big, the parameters might be adjusted too much, missing the minimum of the loss function. If the learning rate is too small, the parameters will be adjusted too little, and the algorithm will take too long to converge.</p>
</section>
<section id="types-of-gradient-descent" class="level1">
<h1>Types of Gradient Descent</h1>
<p>There are two types of gradient descent: batch gradient descent and stochastic gradient descent. Then there is a hybrid of the two called mini-batch gradient descent.</p>
<p><img src="zzgd%20comp.png" class="img-fluid"></p>
<p>To make the explanation easier, I will use the following notation:</p>
<blockquote class="blockquote">
<p>Given that there are <span class="math inline">\(n\)</span> examples in our training data, let <span class="math inline">\(\ell(x^{(i)}, y^{(i)}, \theta)\)</span> be the loss of one example <span class="math inline">\((x^{(i)}, y^{(i)})\)</span> of the training data. Then, <span class="math display">\[J(\theta) = \frac{1}{n}\sum^n_{i=1}\ell(x^{(i)}, y^{(i)}, \theta).\]</span></p>
</blockquote>
<section id="batch-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="batch-gradient-descent">Batch Gradient Descent</h3>
<p>Batch gradient descent is the most straightforward way to implement gradient descent. It averages the gradient over entire (or “batches” of the) dataset: <span class="math display">\[\theta \leftarrow \theta - \alpha \nabla_\theta J(\theta)\]</span></p>
<p>Here are some pros and cons of batch gradient descent:</p>
<p>Pros: - Guaranteed to converge to the global minimum (given enough time) - Easy to implement</p>
<p>Cons: - Slow to converge - Requires a lot of memory</p>
</section>
<section id="stochastic-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-gradient-descent">Stochastic Gradient Descent</h3>
<p>Stochastic gradient descent (SGD) is the opposite of batch gradient descent. Instead of averaging the gradient over the entire dataset, it updates the gradient for every example (iterate over <span class="math inline">\(i = 1, 2, \dots, n\)</span>): <span class="math display">\[\theta \leftarrow \theta - \alpha \nabla_\theta \,\ell(x^{(i)}, y^{(i)}, \theta)\]</span></p>
</section>
<section id="comparison-of-batch-and-stochastic-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="comparison-of-batch-and-stochastic-gradient-descent">Comparison of Batch and Stochastic Gradient Descent</h3>
<table class="table">
<colgroup>
<col style="width: 1%">
<col style="width: 50%">
<col style="width: 47%">
<col style="width: 1%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Batch</th>
<th>Stochastic</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Pros</td>
<td>- stable convergence and error <br> - exploits hardware optimized for matrix computations <br> - more direct path is taken towards the minimum</td>
<td>- scalable to large datasets <br> - memory efficient <br> - computationally cheap to calculate gradient <br> - implicit regularization</td>
<td></td>
</tr>
<tr class="even">
<td>Cons</td>
<td>- computationally expensive to calculate gradient<br> - memory intensive</td>
<td>- high noise in gradient <br> - many updates before convergence <br> - cannot exploit optimized matrix operations</td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="mini-batch-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="mini-batch-gradient-descent">Mini-batch Gradient Descent</h3>
<p>MBGD is a compromise between the above two variations. It has the advantages of both stochastic and batch gradient descent, hence is used most often in practice. It samples a batch of <span class="math inline">\(B\)</span> points <span class="math inline">\(D_B\)</span> at random from the full dataset <span class="math inline">\(D\)</span> without replacement: <span class="math display">\[\theta \leftarrow \theta - \alpha \frac{1}{B}\sum_{\left(x^{(i)},y^{(i)}\right)\in D_B} \nabla_\theta \,\ell(x^{(i)}, y^{(i)}, \theta)\]</span> When <span class="math inline">\(B=1,\)</span> MBGD is just stochastic gradient descent; when <span class="math inline">\(B=n\)</span>, it is batch gradient descent.</p>
</section>
</section>
<section id="closing-thoughts" class="level1">
<h1>Closing Thoughts</h1>
<p>Gradient descent is a very powerful technique that is used in many machine learning algorithms. I hope this post has helped you understand it better. If you have any questions or comments, please leave them below. Thanks for reading!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>