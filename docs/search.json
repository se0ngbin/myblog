[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/cross-entropy-loss/2022-08-06-CrossEntropyLossExplained.html",
    "href": "posts/cross-entropy-loss/2022-08-06-CrossEntropyLossExplained.html",
    "title": "Cross Entropy Loss Explained",
    "section": "",
    "text": "#hide\nfrom fastai.vision.all import *\nCross entropy loss is a loss function that can be used for multi-class classification using neural networks. Chapter 5 of the fast.ai textbook outlines the use of cross entropy loss for binary classification, so in this post, we will take a look at classification for 3 classes."
  },
  {
    "objectID": "posts/arrays-vs-tensors/index.html",
    "href": "posts/arrays-vs-tensors/index.html",
    "title": "Numpy Arrays vs. Pytorch Tensors",
    "section": "",
    "text": "Both NumPy arrays and PyTorch tensors can be viewed as multidimensional tables of data. By using them, we can speed up computations by many thousands of times compared to using pure Python. This post will clarify the difference between the two.\nI will be using the fastai library for this post; the code will be partially based on chapter 4 of the fastai book."
  },
  {
    "objectID": "posts/arrays-vs-tensors/index.html#numpy-arrays",
    "href": "posts/arrays-vs-tensors/index.html#numpy-arrays",
    "title": "Numpy Arrays vs. Pytorch Tensors",
    "section": "NumPy Arrays",
    "text": "NumPy Arrays\nAll items in a NumPy array must be of the same type. Arrays are mutable, which means that we can change the values of each item in the array.\nThe innermost arrays of multidimensional arrays can have varying sizes—this is called a “jagged array.”\nMost functions supported by NumPy arrays are supported by PyTorch tensors."
  },
  {
    "objectID": "posts/arrays-vs-tensors/index.html#pytorch-tensor",
    "href": "posts/arrays-vs-tensors/index.html#pytorch-tensor",
    "title": "Numpy Arrays vs. Pytorch Tensors",
    "section": "Pytorch Tensor",
    "text": "Pytorch Tensor\nAll items in a PyTorch tensor must also be of the same type, but it has the additional restriction that the type has to be a single basic numeric type. Also, PyTorch tensors are immutable, which means that we cannot change the values of each item in the array.\nUnlike arrays, PyTorch tensors cannot be jagged. Also, they can live on the GPU, which is optimized for parallel computations. Therefore, given a large amount of data, it is much faster to use a GPU than a CPU. Additionally, PyTorch can automatically calculate derivatives."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/depression-predictor/2022-09-01-depressionPredictor.html",
    "href": "posts/depression-predictor/2022-09-01-depressionPredictor.html",
    "title": "Reddit Depression Predictor Using Hugging Face Transformers",
    "section": "",
    "text": "This post will cover how to fine tune a NLP classification model using Hugging Face Transformers. I will be using the cleaned reddit depression dataset, which specifies whether or not a post was made in the r/depression subreddit, to train my model.\nThe final model will be able to classify whether or not a block of text was written in the r/depression subreddit with 98% accuracy. I will create a demo to have users input text and see if it is shows signs of depression.\nThe similarity of a block of text to posts in r/depression is not perfectly correlated to the text showing signs of clinical depression, so the accuracy of our demo cannot be quantified. However, it can still provide some insight into what type of texts might have been written by depressed patients.\nCredits go to the hugging face documentation as well as fast.ai, which are both great educational resources."
  },
  {
    "objectID": "posts/digit-classifier/2022-08-05-allDigitsClassify.html",
    "href": "posts/digit-classifier/2022-08-05-allDigitsClassify.html",
    "title": "MNIST Digit Classifier",
    "section": "",
    "text": "This post will cover how to classify handwritten digits of the MNIST dataset using a simple neural network. At the same time, I will be taking a stab at the Kaggle Digit Recognizer contest.\nCredits: I will be working off of chapter 4 of the fast.ai book, which covers binary classification of 3’s and 7’s. Other resources are linked."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "seong/bin/blog",
    "section": "",
    "text": "Welcome to my blog. I will be posting some interesting and uninteresting things here, but hopefully every post will be of some value to you."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m interested in the start up space, backend engineering, and machine learning1. My goal is to establish a successful start up backed by powerful ideas and machine learning models.\nCheck out my website for a portfolio.\n\n\n\nFootnotes\n\n\nthe fast.ai course helped me a lot on my machine learning journey; go check it out!↩︎"
  },
  {
    "objectID": "posts/cross-entropy-loss/2022-08-06-CrossEntropyLossExplained.html#softmax",
    "href": "posts/cross-entropy-loss/2022-08-06-CrossEntropyLossExplained.html#softmax",
    "title": "Cross Entropy Loss Explained",
    "section": "Softmax",
    "text": "Softmax\nThe softmax function ensures 2 things: - activations are all between 0 and 1 - activations sum to 1.\nFor multi-class classification, we need an activation per class (in the final layer). Each activation then indicates the relative confidence of each class being the true label. Therefore, we can get the predicted prababilities that each class is the true label by applying the softmax function to the final column of activations.\nGiven \\(C\\) total classes, for any class \\(k,\\) let’s say \\(x_k\\) represents the activation for \\(c\\). Then, the softmax activation for an arbitrary class \\(c\\) is equal to\n\\[\\frac{e^{x_c}}{\\sum^C_{k=1}e^{x_k}}.\\]\nIn Python code, this would be\ndef softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True)\nNote that the code version returns a tensor/array of softmax activations.\nFor demonstration purposes, let’s first create a set of activations using torch.randn, assuming we have 6 objects to classify into 3 classes.\n\n#hide\ntorch.random.manual_seed(42);\n\n\nacts = torch.randn((6,3))*2\nacts\n\ntensor([[ 3.8538,  2.9746, -0.9948],\n        [ 0.8792, -1.5163,  2.1566],\n        [ 1.6016,  3.3612,  0.7117],\n        [-1.3732,  1.2209,  2.6695],\n        [-0.4632,  0.0835, -0.5032],\n        [ 1.7197, -0.6195, -0.7914]])\n\n\nLet’s also set our target labels:\n\ntarg = tensor([0,1,0,2,2,0])\n\nTo take the softmax of our initial (random) activations, we need to pass acts into torch.softmax:\n\nsm_acts = torch.softmax(acts, dim=1)\nsm_acts\n\ntensor([[0.7028, 0.2917, 0.0055],\n        [0.2137, 0.0195, 0.7668],\n        [0.1385, 0.8046, 0.0569],\n        [0.0140, 0.1876, 0.7984],\n        [0.2711, 0.4684, 0.2605],\n        [0.8492, 0.0819, 0.0689]])\n\n\nPerfect! We can check that each row adds up to 1 as expected."
  },
  {
    "objectID": "posts/cross-entropy-loss/2022-08-06-CrossEntropyLossExplained.html#log-likelihood",
    "href": "posts/cross-entropy-loss/2022-08-06-CrossEntropyLossExplained.html#log-likelihood",
    "title": "Cross Entropy Loss Explained",
    "section": "Log Likelihood",
    "text": "Log Likelihood\nTo calculate our loss, for each item of targ, we need to select the appropriate column of sm_acts using tensor indexing:\n\nidx = range(6)\nsm_acts[idx, targ]\n\ntensor([0.7028, 0.0195, 0.1385, 0.7984, 0.2605, 0.8492])\n\n\nF.nll_loss does the same thing, but flips the sign of each number in the tensor. PyTorch defaults to taking the mean of the losses; to prevent this, we can pass reduction='none' as a parameter.\n\nresult = -F.nll_loss(sm_acts, targ, reduction='none')\nresult\n\ntensor([0.7028, 0.0195, 0.1385, 0.7984, 0.2605, 0.8492])\n\n\n\nTaking the Logarithm\nWe take the (natural) logarithm of result for two reasons: - prevents under/overflow when performing mathematical operations - differences between small numbers is amplified\nIn our case, result relfects the predicted probability of the correct label, so when the prediction is “good” (closer to 1), we want our loss function to return a small value (and vice versa). We can achieve this by taking the negative of the log:\n\nloss = -torch.log(result)\nloss\n\ntensor([0.3527, 3.9384, 1.9770, 0.2251, 1.3451, 0.1635])\n\n\nAnd there we go! We just found the cross entropy loss for our example."
  },
  {
    "objectID": "posts/cross-entropy-loss/index.html",
    "href": "posts/cross-entropy-loss/index.html",
    "title": "Cross Entropy Loss Explained",
    "section": "",
    "text": "Cross entropy loss is a loss function that can be used for multi-class classification using neural networks. Chapter 5 of the fast.ai textbook outlines the use of cross entropy loss for binary classification, so in this post, we will take a look at classification for 3 classes."
  },
  {
    "objectID": "posts/cross-entropy-loss/index.html#softmax",
    "href": "posts/cross-entropy-loss/index.html#softmax",
    "title": "Cross Entropy Loss Explained",
    "section": "Softmax",
    "text": "Softmax\nThe softmax function ensures 2 things: - activations are all between 0 and 1 - activations sum to 1.\nFor multi-class classification, we need an activation per class (in the final layer). Each activation then indicates the relative confidence of each class being the true label. Therefore, we can get the predicted prababilities that each class is the true label by applying the softmax function to the final column of activations.\nGiven \\(C\\) total classes, for any class \\(k,\\) let’s say \\(x_k\\) represents the activation for \\(c\\). Then, the softmax activation for an arbitrary class \\(c\\) is equal to\n\\[\\frac{e^{x_c}}{\\sum^C_{k=1}e^{x_k}}.\\]\nIn Python code, this would be\ndef softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True)\nNote that the code version returns a tensor/array of softmax activations.\nFor demonstration purposes, let’s first create a set of activations using torch.randn, assuming we have 6 objects to classify into 3 classes.\n\nacts = torch.randn((6,3))*2\nacts\n\ntensor([[ 3.8538,  2.9746, -0.9948],\n        [ 0.8792, -1.5163,  2.1566],\n        [ 1.6016,  3.3612,  0.7117],\n        [-1.3732,  1.2209,  2.6695],\n        [-0.4632,  0.0835, -0.5032],\n        [ 1.7197, -0.6195, -0.7914]])\n\n\nLet’s also set our target labels:\n\ntarg = tensor([0,1,0,2,2,0])\n\nTo take the softmax of our initial (random) activations, we need to pass acts into torch.softmax:\n\nsm_acts = torch.softmax(acts, dim=1)\nsm_acts\n\ntensor([[0.7028, 0.2917, 0.0055],\n        [0.2137, 0.0195, 0.7668],\n        [0.1385, 0.8046, 0.0569],\n        [0.0140, 0.1876, 0.7984],\n        [0.2711, 0.4684, 0.2605],\n        [0.8492, 0.0819, 0.0689]])\n\n\nPerfect! We can check that each row adds up to 1 as expected."
  },
  {
    "objectID": "posts/cross-entropy-loss/index.html#log-likelihood",
    "href": "posts/cross-entropy-loss/index.html#log-likelihood",
    "title": "Cross Entropy Loss Explained",
    "section": "Log Likelihood",
    "text": "Log Likelihood\nTo calculate our loss, for each item of targ, we need to select the appropriate column of sm_acts using tensor indexing:\n\nidx = range(6)\nsm_acts[idx, targ]\n\ntensor([0.7028, 0.0195, 0.1385, 0.7984, 0.2605, 0.8492])\n\n\nF.nll_loss does the same thing, but flips the sign of each number in the tensor. PyTorch defaults to taking the mean of the losses; to prevent this, we can pass reduction='none' as a parameter.\n\nresult = -F.nll_loss(sm_acts, targ, reduction='none')\nresult\n\ntensor([0.7028, 0.0195, 0.1385, 0.7984, 0.2605, 0.8492])\n\n\n\nTaking the Logarithm\nWe take the (natural) logarithm of result for two reasons: - prevents under/overflow when performing mathematical operations - differences between small numbers is amplified\nIn our case, result relfects the predicted probability of the correct label, so when the prediction is “good” (closer to 1), we want our loss function to return a small value (and vice versa). We can achieve this by taking the negative of the log:\n\nloss = -torch.log(result)\nloss\n\ntensor([0.3527, 3.9384, 1.9770, 0.2251, 1.3451, 0.1635])\n\n\nAnd there we go! We just found the cross entropy loss for our example."
  },
  {
    "objectID": "posts/depression-predictor/2022-09-01-depressionPredictor.html#obtaining-data",
    "href": "posts/depression-predictor/2022-09-01-depressionPredictor.html#obtaining-data",
    "title": "Reddit Depression Predictor Using Hugging Face Transformers",
    "section": "Obtaining Data",
    "text": "Obtaining Data\nFirst, I will fetch the dataset using opendatasets:\n\ntry:\n    import opendatasets as od\nexcept:\n    !pip install opendatasets\n    import opendatasets as od\n\n\nfrom pathlib import Path\n\npath = Path(\"depression-reddit-cleaned\")\n\n\nif not path.exists():\n    od.download(\"https://www.kaggle.com/datasets/infamouscoder/depression-reddit-cleaned\")"
  },
  {
    "objectID": "posts/depression-predictor/index.html",
    "href": "posts/depression-predictor/index.html",
    "title": "Reddit Depression Predictor Using Hugging Face Transformers",
    "section": "",
    "text": "This post will cover how to fine tune a NLP classification model using Hugging Face Transformers. I will be using the cleaned reddit depression dataset, which specifies whether or not a post was made in the r/depression subreddit, to train my model.\nThe final model will be able to classify whether or not a block of text was written in the r/depression subreddit with 98% accuracy. I will create a demo to have users input text and see if it is shows signs of depression.\nThe similarity of a block of text to posts in r/depression is not perfectly correlated to the text showing signs of clinical depression, so the accuracy of our demo cannot be quantified. However, it can still provide some insight into what type of texts might have been written by depressed patients.\nCredits go to the hugging face documentation as well as fast.ai, which are both great educational resources."
  },
  {
    "objectID": "posts/depression-predictor/index.html#obtaining-data",
    "href": "posts/depression-predictor/index.html#obtaining-data",
    "title": "Reddit Depression Predictor Using Hugging Face Transformers",
    "section": "Obtaining Data",
    "text": "Obtaining Data\nFirst, I will fetch the dataset using opendatasets:\n\ntry:\n    import opendatasets as od\nexcept:\n    !pip install opendatasets\n    import opendatasets as od\n\n\nfrom pathlib import Path\n\npath = Path(\"depression-reddit-cleaned\")\n\n\nif not path.exists():\n    od.download(\"https://www.kaggle.com/datasets/infamouscoder/depression-reddit-cleaned\")"
  },
  {
    "objectID": "posts/digit-classifier/2022-08-05-allDigitsClassify.html#dowloading-the-data",
    "href": "posts/digit-classifier/2022-08-05-allDigitsClassify.html#dowloading-the-data",
    "title": "MNIST Digit Classifier",
    "section": "Dowloading the Data",
    "text": "Dowloading the Data\nFirst, we will have to import the MNIST dataset itself. We can import it using the fast.ai library (path = untar_data(URLs.MNIST)), but I will download the dataset from kaggle instead.\nIf you are following along and haven’t set up the kaggle API yet, do so by following along the README of the official repo. You will need an account to do so. After everything is set up, we can run the following code block:\n\n!kaggle competitions download -c digit-recognizer\n\nDownloading digit-recognizer.zip to /home/jupyter/projects/digit-classifier\n  0%|                                               | 0.00/15.3M [00:00<?, ?B/s]\n100%|███████████████████████████████████████| 15.3M/15.3M [00:00<00:00, 161MB/s]\n\n\nNote that in Jupyter notebooks, the exclamation mark ! is used to execute shell commands. The dataset should be downloaded in your project directory as a zip file. Run the following code block to extract the contents to a file named MNIST_dataset:\n\n!unzip digit-recognizer.zip -d MNIST_dataset\n\nArchive:  digit-recognizer.zip\n  inflating: MNIST_dataset/sample_submission.csv  \n  inflating: MNIST_dataset/test.csv  \n  inflating: MNIST_dataset/train.csv  \n\n\nLet’s take a look at test.csv (the test set) and train.csv (the training set):\n\nds_path = Path(\"./MNIST_dataset\")\n\n\n# test.csv\ndf_test = pd.read_csv(ds_path/\"test.csv\")\ndf_test.head(3)\n\n\n\n\n\n  \n    \n      \n      pixel0\n      pixel1\n      pixel2\n      pixel3\n      pixel4\n      pixel5\n      pixel6\n      pixel7\n      pixel8\n      pixel9\n      ...\n      pixel774\n      pixel775\n      pixel776\n      pixel777\n      pixel778\n      pixel779\n      pixel780\n      pixel781\n      pixel782\n      pixel783\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n3 rows × 784 columns\n\n\n\n\n# train.csv\ndf_train = pd.read_csv(ds_path/\"train.csv\")\ndf_train.head(3)\n\n\n\n\n\n  \n    \n      \n      label\n      pixel0\n      pixel1\n      pixel2\n      pixel3\n      pixel4\n      pixel5\n      pixel6\n      pixel7\n      pixel8\n      ...\n      pixel774\n      pixel775\n      pixel776\n      pixel777\n      pixel778\n      pixel779\n      pixel780\n      pixel781\n      pixel782\n      pixel783\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n3 rows × 785 columns\n\n\n\nNow that we downloaded the data, we need to shape it for training and validating."
  },
  {
    "objectID": "posts/digit-classifier/2022-08-05-allDigitsClassify.html#shaping-the-data",
    "href": "posts/digit-classifier/2022-08-05-allDigitsClassify.html#shaping-the-data",
    "title": "MNIST Digit Classifier",
    "section": "Shaping the Data",
    "text": "Shaping the Data\nTo train our model, we need to separate and normalize the independent (pixels) and dependent (label) variables. The labels will be represented using one hot encoding.\n\nX_train = tensor(df_train.drop(labels = ['label'],axis = 1)) / 255.0\n\n\ny_train_numeric = df_train['label']\n\nrows = np.arange(y_train_numeric.size)\ny_train = tensor(np.zeros((y_train_numeric.size, 10)))\ny_train[rows, y_train_numeric] = 1\n\nX_train.shape, y_train.shape\n\n(torch.Size([42000, 784]), torch.Size([42000, 10]))\n\n\nX_train.shape and y_train.shape tells us that we have 42000 digits in our dataset, with each digit having 784 pixels. We will use tensors to take advantage of faster GPU computations.\nWe want to create a Pytorch Dataset, which is required to return a tuple of (x,y) when indexed. Python provides a zip function which, when combined with list, can do this easily:\n\nds = list(zip(X_train,y_train))\nds[0]\n\n(tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7373, 1.0000, 0.3686,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7490, 0.9804, 0.9922,\n         0.3647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4824, 0.9725, 0.9922,\n         0.6549, 0.0392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3137, 0.9686, 0.9922,\n         0.8157, 0.0510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1137, 0.8118, 0.9922,\n         0.9216, 0.3020, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2118, 0.8196, 0.9922,\n         0.9922, 0.3451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3647, 0.9961, 0.9922,\n         0.9333, 0.6667, 0.0667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0902, 0.8235, 0.9961,\n         0.9922, 0.6235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627, 0.8196, 0.9922,\n         0.9961, 0.9412, 0.3176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1059, 0.9922,\n         0.9922, 0.9961, 0.0510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0784, 0.8078,\n         0.9961, 0.9961, 0.7765, 0.0275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6588,\n         0.9922, 0.9922, 0.7686, 0.0275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0784,\n         0.7961, 0.9922, 0.9725, 0.2980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0863,\n         0.7373, 0.9922, 0.9608, 0.3647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.4039, 0.9922, 0.9922, 0.7490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.3490, 0.9412, 0.9922, 0.7647, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0588, 0.8627, 0.9922, 0.9922, 0.3137, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.3686, 0.9922, 0.9922, 0.9922, 0.3686, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.3490, 0.9843, 0.9922, 0.9804, 0.5137, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.8392, 0.8549, 0.3725, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000]),\n tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]))\n\n\nNext, we want to split our dataset ds into a training and validation set:\n\ntrain, val = torch.utils.data.random_split(ds,[32000, 10000])\n\nLater, we will be using stochastic gradient descent, which requires that we have “mini-batches” of our dataset. We can create a DataLoader from our train dataset to do so:\n\ndl = DataLoader(train, batch_size=256)\nxb,yb = first(dl)\nxb.shape,yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 10]))\n\n\nWe can do the same for our validation (val) dataset:\n\nvalid_dl = DataLoader(val, batch_size=256)"
  },
  {
    "objectID": "posts/digit-classifier/2022-08-05-allDigitsClassify.html#training-a-linear-model",
    "href": "posts/digit-classifier/2022-08-05-allDigitsClassify.html#training-a-linear-model",
    "title": "MNIST Digit Classifier",
    "section": "Training a Linear Model",
    "text": "Training a Linear Model\nNow that our data is ready, we can start training our classification model. We will start with a linear model, then add some non-linearity to it!\nFirst, we must randomly initialize the bias and all weights for each pixel. Since we have 10 labels (one for each digit), there must be 10 outputs, so our weights matrix is of size 784x10.\n\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\nweights = init_params((784,10))\nbias = init_params(1)\n\nThe prediction given a tensor x is\n\\[\\text{prediction} = x \\cdot \\text{weights} + \\text{bias}.\\]\n\ndef linear1(xb): return xb@weights + bias\n\nTo calculate a gradient, we need a loss function. Since there are more than 2 labels, we will use cross entropy loss, which is related to the softmax function instead of a sigmoid function (which is used for binary classification).\n\ndef mnist_loss(xb, yb):\n    loss = nn.CrossEntropyLoss()\n    return loss(xb, yb)\n\nFor testing and demonstration purposes, let’s work with a smaller batch than the ones created when shaping our data.\n\nbatch = X_train[:4]\nbatch.shape\n\ntorch.Size([4, 784])\n\n\n\npreds = linear1(batch)\npreds\n\ntensor([[ -0.5287,  12.6047,  -9.0290,  -1.7505,   3.7686,  18.8489,  -1.8141,\n         -12.2232,  -5.1421,  -6.6316],\n        [ 18.8942,  10.7898,   9.2573,   7.9989,  -1.2884,  19.0238,  -5.8788,\n           6.5045, -10.2431,   5.5865],\n        [  6.3639,  14.0687,   0.7705,  -1.3580,   1.4220,   7.3108,  -7.4359,\n          -6.8101,  -5.9212,  23.7016],\n        [-14.7847,   3.0711,  -0.6092,   2.2720,  -1.1361,   3.7617,   5.1197,\n           5.3868,  -1.5228,  -7.6523]], grad_fn=<AddBackward0>)\n\n\n\nloss = mnist_loss(preds, y_train[:4])\nloss\n\ntensor(5.9773, grad_fn=<DivBackward1>)\n\n\nNow we can calculate the gradients:\n\nloss.backward()\nweights.grad.shape,weights.grad.mean(),bias.grad\n\n(torch.Size([784, 10]), tensor(2.4328e-10), tensor([5.9605e-08]))\n\n\nThe following function combines the above code and generalizes to models other than linear1.\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\n\ncalc_grad(batch, y_train[:4], linear1)\nweights.grad.mean(),bias.grad\n\n(tensor(4.8657e-10), tensor([1.1921e-07]))\n\n\nUsing the calculated gradients, we can update the weights for each epoch. We need to specify a learning rate and reset the gradients to 0, since loss.backward actually adds the gradients of loss to any gradients that are currently stored.\nlr = 1.\n\nfor p in weights,bias:\n    p.data -= p.grad*lr\n    p.grad.zero_()\nFinally, we can define a function that trains the model for one epoch:\n\ndef train_epoch(model, params, lr=1):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\nWe also probably want to check the accuracy of our model. The label that the model predicts is the label with the highest activation:\n\ndef batch_accuracy(xb, yb):\n    label = torch.argmax(xb, dim=1)\n    y_truth = torch.argmax(yb, dim=1)\n    correct = y_truth == label\n    return correct.float().mean()\n\n\nbatch_accuracy(linear1(batch), y_train[:4])\n\ntensor(0.)\n\n\nTo get the accuracy for the whole epoch, we must call batch_accuracy with batches of the validation dataset, then take the mean over all batches.\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\nFinally, we can see if our code works by checking if the accuracy improves!\n\nparams = weights, bias\nfor i in range(40):\n    train_epoch(linear1, params)\n    print(validate_epoch(linear1), end=' ')\n\n0.8213 0.8532 0.8661 0.8736 0.8769 0.8808 0.8848 0.8869 0.8895 0.8914 0.8929 0.894 0.8949 0.8964 0.8973 0.8977 0.8978 0.8979 0.8981 0.8986 0.8998 0.9 0.9004 0.9016 0.9017 0.9027 0.9027 0.9032 0.9037 0.9047 0.9053 0.9058 0.9061 0.9062 0.9061 0.9062 0.906 0.9061 0.906 0.9063"
  },
  {
    "objectID": "posts/digit-classifier/2022-08-05-allDigitsClassify.html#simplifying-code",
    "href": "posts/digit-classifier/2022-08-05-allDigitsClassify.html#simplifying-code",
    "title": "MNIST Digit Classifier",
    "section": "Simplifying Code",
    "text": "Simplifying Code\nnn.Linear does the same thing as our init_params and linear1 functions together. Also, fastai’s SGD class provides us with functions that takes care of updating the parameters and reseting the gradients of our model. By replacing some code, we can boil the training portion of our MNIST classifer down to the following:\n\ndef mnist_loss(xb, yb):\n    loss = nn.CrossEntropyLoss()\n    return loss(xb, yb)\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\ndef train_epoch_simple(model):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n        \ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch_simple(model)\n        print(validate_epoch(model), end=' ')\n        \nlinear_model = nn.Linear(28*28,10)\nopt = SGD(linear_model.parameters(), lr=1)\ntrain_model(linear_model, 20)\n\n0.8983 0.9057 0.9092 0.9113 0.9143 0.9149 0.9154 0.9154 0.9162 0.9161 0.9166 0.9167 0.9166 0.9166 0.9164 0.9169 0.917 0.9173 0.9173 0.9175 \n\n\nFast.ai provides us with Learner.fit, which we can use instead of train_model to significantly reduce the amount of code we need to write. To use the function, we must create a Learner, which requires a DataLoaders of our training and validation datasets:\n\ndls = DataLoaders(dl, valid_dl)\n\nThen, we pass in DataLoaders, the model, the optimization function, the loss function, and optionally any metrics to print into the Learner constructor to create one:\n\nlearn = Learner(dls, nn.Linear(28*28,10), opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\nFinally, we can call Learner.fit:\n\nlearn.fit(10, lr=1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      batch_accuracy\n      time\n    \n  \n  \n    \n      0\n      0.413918\n      0.365917\n      0.897400\n      00:00\n    \n    \n      1\n      0.326721\n      0.337396\n      0.905100\n      00:00\n    \n    \n      2\n      0.302027\n      0.325679\n      0.908500\n      00:00\n    \n    \n      3\n      0.289524\n      0.319092\n      0.910000\n      00:00\n    \n    \n      4\n      0.281248\n      0.314855\n      0.912800\n      00:00\n    \n    \n      5\n      0.275084\n      0.311911\n      0.913200\n      00:00\n    \n    \n      6\n      0.270192\n      0.309764\n      0.913500\n      00:00\n    \n    \n      7\n      0.266153\n      0.308146\n      0.913900\n      00:00\n    \n    \n      8\n      0.262723\n      0.306901\n      0.914300\n      00:00\n    \n    \n      9\n      0.259748\n      0.305928\n      0.914900\n      00:00"
  },
  {
    "objectID": "posts/digit-classifier/2022-08-05-allDigitsClassify.html#adding-non-linearity",
    "href": "posts/digit-classifier/2022-08-05-allDigitsClassify.html#adding-non-linearity",
    "title": "MNIST Digit Classifier",
    "section": "Adding Non-linearity",
    "text": "Adding Non-linearity\nTo expand upon our model, we can add another layer on top of what we have now. However, mathematically speaking, the composition of two linear functions is another linear function. Therefore, stacking two linear classifiers on top of each other is equivalent to having just one linear classifier.\nTherefore, we must add some non-linearity between linear layers. We often do this by through activation functions; a common one is the ReLU function:\n\nx = np.linspace(-5,5,100)\ny = np.maximum(0, x)\n\nplt.plot(x, y)\n\n\n\n\nnn.Sequential creates a module that will call each of the listed layers or functions.\nOur first layer takes in 784 inputs (pixels) and outputs 60 numbers. Those 60 numbers are then each passed into the ReLU function before going into the second layer. The second layer has 10 outputs, which as before, is the probability of each digit being the lable.\n\nsimple_net = nn.Sequential(\n    nn.Linear(784,100),\n    nn.ReLU(),\n    nn.Linear(100,10)\n)\n\nWe can train this model using Learner.fit as well (we are using more epochs and smaller learning rate, since it is a larger model):\n\n#hide_output\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\nlearn.fit(60, lr=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      batch_accuracy\n      time\n    \n  \n  \n    \n      0\n      0.781461\n      0.532242\n      0.866300\n      00:00\n    \n    \n      1\n      0.441000\n      0.395819\n      0.893000\n      00:00\n    \n    \n      2\n      0.355962\n      0.352042\n      0.902600\n      00:00\n    \n    \n      3\n      0.319263\n      0.327102\n      0.909500\n      00:00\n    \n    \n      4\n      0.295532\n      0.308874\n      0.913600\n      00:00\n    \n    \n      5\n      0.276869\n      0.293574\n      0.917000\n      00:00\n    \n    \n      6\n      0.261044\n      0.280591\n      0.920400\n      00:00\n    \n    \n      7\n      0.247288\n      0.268860\n      0.924300\n      00:00\n    \n    \n      8\n      0.235076\n      0.258463\n      0.927900\n      00:00\n    \n    \n      9\n      0.223990\n      0.248834\n      0.930800\n      00:00\n    \n    \n      10\n      0.213890\n      0.240252\n      0.932900\n      00:00\n    \n    \n      11\n      0.204643\n      0.232565\n      0.935200\n      00:00\n    \n    \n      12\n      0.196180\n      0.225423\n      0.937000\n      00:00\n    \n    \n      13\n      0.188346\n      0.218758\n      0.939200\n      00:00\n    \n    \n      14\n      0.181094\n      0.212594\n      0.939900\n      00:00\n    \n    \n      15\n      0.174359\n      0.206947\n      0.940100\n      00:00\n    \n    \n      16\n      0.168056\n      0.201667\n      0.941700\n      00:00\n    \n    \n      17\n      0.162067\n      0.196636\n      0.943100\n      00:00\n    \n    \n      18\n      0.156417\n      0.191967\n      0.944300\n      00:00\n    \n    \n      19\n      0.151026\n      0.187445\n      0.945500\n      00:00\n    \n    \n      20\n      0.145904\n      0.183263\n      0.946400\n      00:00\n    \n    \n      21\n      0.141032\n      0.179306\n      0.947400\n      00:00\n    \n    \n      22\n      0.136399\n      0.175533\n      0.948500\n      00:00\n    \n    \n      23\n      0.132011\n      0.171847\n      0.949300\n      00:00\n    \n    \n      24\n      0.127819\n      0.168481\n      0.950700\n      00:00\n    \n    \n      25\n      0.123817\n      0.165292\n      0.951900\n      00:00\n    \n    \n      26\n      0.120019\n      0.162331\n      0.952500\n      00:00\n    \n    \n      27\n      0.116390\n      0.159504\n      0.953500\n      00:00\n    \n    \n      28\n      0.112947\n      0.156784\n      0.954100\n      00:00\n    \n    \n      29\n      0.109665\n      0.154226\n      0.955200\n      00:00\n    \n    \n      30\n      0.106508\n      0.151784\n      0.955900\n      00:00\n    \n    \n      31\n      0.103485\n      0.149547\n      0.956500\n      00:00\n    \n    \n      32\n      0.100609\n      0.147393\n      0.957300\n      00:00\n    \n    \n      33\n      0.097871\n      0.145380\n      0.957700\n      00:00\n    \n    \n      34\n      0.095229\n      0.143457\n      0.958700\n      00:00\n    \n    \n      35\n      0.092694\n      0.141595\n      0.959100\n      00:00\n    \n    \n      36\n      0.090273\n      0.139827\n      0.959100\n      00:00\n    \n    \n      37\n      0.087953\n      0.138202\n      0.959800\n      00:00\n    \n    \n      38\n      0.085736\n      0.136662\n      0.960100\n      00:00\n    \n    \n      39\n      0.083610\n      0.135193\n      0.960400\n      00:00\n    \n    \n      40\n      0.081549\n      0.133738\n      0.960800\n      00:00\n    \n    \n      41\n      0.079560\n      0.132337\n      0.961600\n      00:00\n    \n    \n      42\n      0.077661\n      0.131035\n      0.962100\n      00:00\n    \n    \n      43\n      0.075826\n      0.129802\n      0.962100\n      00:00\n    \n    \n      44\n      0.074042\n      0.128692\n      0.962200\n      00:00\n    \n    \n      45\n      0.072344\n      0.127533\n      0.962300\n      00:00\n    \n    \n      46\n      0.070702\n      0.126541\n      0.962500\n      00:00\n    \n    \n      47\n      0.069096\n      0.125475\n      0.963100\n      00:00\n    \n    \n      48\n      0.067572\n      0.124556\n      0.963300\n      00:00\n    \n    \n      49\n      0.066088\n      0.123669\n      0.963600\n      00:00\n    \n    \n      50\n      0.064652\n      0.122781\n      0.963900\n      00:00\n    \n    \n      51\n      0.063260\n      0.121974\n      0.964200\n      00:00\n    \n    \n      52\n      0.061913\n      0.121199\n      0.964300\n      00:00\n    \n    \n      53\n      0.060608\n      0.120421\n      0.964600\n      00:00\n    \n    \n      54\n      0.059330\n      0.119673\n      0.964700\n      00:00\n    \n    \n      55\n      0.058102\n      0.118998\n      0.965000\n      00:01\n    \n    \n      56\n      0.056910\n      0.118366\n      0.965000\n      00:00\n    \n    \n      57\n      0.055744\n      0.117742\n      0.965000\n      00:00\n    \n    \n      58\n      0.054619\n      0.117166\n      0.965100\n      00:00\n    \n    \n      59\n      0.053524\n      0.116611\n      0.965200\n      00:00\n    \n  \n\n\n\nThe output is ommitted to save room; the training process is recorded in learn.recorder, with the table of output stored in the values attribute, so we can plot the accuracy over training as:\n\nplt.plot(L(learn.recorder.values).itemgot(2))\n\n\n\n\n\n# final accuracy\nlearn.recorder.values[-1][2]\n\n0.9652000069618225"
  },
  {
    "objectID": "posts/digit-classifier/2022-08-05-allDigitsClassify.html#making-a-submission",
    "href": "posts/digit-classifier/2022-08-05-allDigitsClassify.html#making-a-submission",
    "title": "MNIST Digit Classifier",
    "section": "Making a Submission",
    "text": "Making a Submission\nThough our very basic model is far from perfect, we can still submit it to the competition! Recall that we stored the test.csv data into the df_test DataFrame. We need to first normalize the pixels then plug it into our model:\n\nX_test = tensor(df_test)/ 255.0\n\n\nx_val = simple_net(X_test)\ny_pred = torch.argmax(x_val, dim=1)\ny_pred = y_pred.tolist()\n\nFinally, we can create a submission file in our current directory:\n\nwith open(\"submission.csv\", 'w+') as f :\n    f.write('ImageId,Label\\n')\n    for i in range(len(y_pred)) :\n        f.write(\"\".join([str(i+1),',',str(y_pred[i]),'\\n']))\n\nThen submit to Kaggle!\n\n!kaggle competitions submit -c digit-recognizer -f submission.csv -m \"First Attempt\"\n\n100%|█████████████████████████████████████████| 208k/208k [00:01<00:00, 165kB/s]\nSuccessfully submitted to Digit Recognizer"
  },
  {
    "objectID": "posts/digit-classifier/index.html",
    "href": "posts/digit-classifier/index.html",
    "title": "MNIST Digit Classifier",
    "section": "",
    "text": "This post will cover how to classify handwritten digits of the MNIST dataset using a simple neural network. At the same time, I will be taking a stab at the Kaggle Digit Recognizer contest.\nCredits: I will be working off of chapter 4 of the fast.ai book, which covers binary classification of 3’s and 7’s. Other resources are linked."
  },
  {
    "objectID": "posts/digit-classifier/index.html#dowloading-the-data",
    "href": "posts/digit-classifier/index.html#dowloading-the-data",
    "title": "MNIST Digit Classifier",
    "section": "Dowloading the Data",
    "text": "Dowloading the Data\nFirst, we will have to import the MNIST dataset itself. We can import it using the fast.ai library (path = untar_data(URLs.MNIST)), but I will download the dataset from kaggle instead.\nIf you are following along and haven’t set up the kaggle API yet, do so by following along the README of the official repo. You will need an account to do so. After everything is set up, we can run the following code block:\n\n!kaggle competitions download -c digit-recognizer\n\nDownloading digit-recognizer.zip to /home/jupyter/projects/digit-classifier\n  0%|                                               | 0.00/15.3M [00:00<?, ?B/s]\n100%|███████████████████████████████████████| 15.3M/15.3M [00:00<00:00, 161MB/s]\n\n\nNote that in Jupyter notebooks, the exclamation mark ! is used to execute shell commands. The dataset should be downloaded in your project directory as a zip file. Run the following code block to extract the contents to a file named MNIST_dataset:\n\n!unzip digit-recognizer.zip -d MNIST_dataset\n\nArchive:  digit-recognizer.zip\n  inflating: MNIST_dataset/sample_submission.csv  \n  inflating: MNIST_dataset/test.csv  \n  inflating: MNIST_dataset/train.csv  \n\n\nLet’s take a look at test.csv (the test set) and train.csv (the training set):\n\nds_path = Path(\"./MNIST_dataset\")\n\n\n# test.csv\ndf_test = pd.read_csv(ds_path/\"test.csv\")\ndf_test.head(3)\n\n\n\n\n\n  \n    \n      \n      pixel0\n      pixel1\n      pixel2\n      pixel3\n      pixel4\n      pixel5\n      pixel6\n      pixel7\n      pixel8\n      pixel9\n      ...\n      pixel774\n      pixel775\n      pixel776\n      pixel777\n      pixel778\n      pixel779\n      pixel780\n      pixel781\n      pixel782\n      pixel783\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n3 rows × 784 columns\n\n\n\n\n# train.csv\ndf_train = pd.read_csv(ds_path/\"train.csv\")\ndf_train.head(3)\n\n\n\n\n\n  \n    \n      \n      label\n      pixel0\n      pixel1\n      pixel2\n      pixel3\n      pixel4\n      pixel5\n      pixel6\n      pixel7\n      pixel8\n      ...\n      pixel774\n      pixel775\n      pixel776\n      pixel777\n      pixel778\n      pixel779\n      pixel780\n      pixel781\n      pixel782\n      pixel783\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n3 rows × 785 columns\n\n\n\nNow that we downloaded the data, we need to shape it for training and validating."
  },
  {
    "objectID": "posts/digit-classifier/index.html#shaping-the-data",
    "href": "posts/digit-classifier/index.html#shaping-the-data",
    "title": "MNIST Digit Classifier",
    "section": "Shaping the Data",
    "text": "Shaping the Data\nTo train our model, we need to separate and normalize the independent (pixels) and dependent (label) variables. The labels will be represented using one hot encoding.\n\nX_train = tensor(df_train.drop(labels = ['label'],axis = 1)) / 255.0\n\n\ny_train_numeric = df_train['label']\n\nrows = np.arange(y_train_numeric.size)\ny_train = tensor(np.zeros((y_train_numeric.size, 10)))\ny_train[rows, y_train_numeric] = 1\n\nX_train.shape, y_train.shape\n\n(torch.Size([42000, 784]), torch.Size([42000, 10]))\n\n\nX_train.shape and y_train.shape tells us that we have 42000 digits in our dataset, with each digit having 784 pixels. We will use tensors to take advantage of faster GPU computations.\nWe want to create a Pytorch Dataset, which is required to return a tuple of (x,y) when indexed. Python provides a zip function which, when combined with list, can do this easily:\n\nds = list(zip(X_train,y_train))\nds[0]\n# output removed for readability\n\nNext, we want to split our dataset ds into a training and validation set:\n\ntrain, val = torch.utils.data.random_split(ds,[32000, 10000])\n\nLater, we will be using stochastic gradient descent, which requires that we have “mini-batches” of our dataset. We can create a DataLoader from our train dataset to do so:\n\ndl = DataLoader(train, batch_size=256)\nxb,yb = first(dl)\nxb.shape,yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 10]))\n\n\nWe can do the same for our validation (val) dataset:\n\nvalid_dl = DataLoader(val, batch_size=256)"
  },
  {
    "objectID": "posts/digit-classifier/index.html#training-a-linear-model",
    "href": "posts/digit-classifier/index.html#training-a-linear-model",
    "title": "MNIST Digit Classifier",
    "section": "Training a Linear Model",
    "text": "Training a Linear Model\nNow that our data is ready, we can start training our classification model. We will start with a linear model, then add some non-linearity to it!\nFirst, we must randomly initialize the bias and all weights for each pixel. Since we have 10 labels (one for each digit), there must be 10 outputs, so our weights matrix is of size 784x10.\n\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\nweights = init_params((784,10))\nbias = init_params(1)\n\nThe prediction given a tensor x is\n\\[\\text{prediction} = x \\cdot \\text{weights} + \\text{bias}.\\]\n\ndef linear1(xb): return xb@weights + bias\n\nTo calculate a gradient, we need a loss function. Since there are more than 2 labels, we will use cross entropy loss, which is related to the softmax function instead of a sigmoid function (which is used for binary classification).\n\ndef mnist_loss(xb, yb):\n    loss = nn.CrossEntropyLoss()\n    return loss(xb, yb)\n\nFor testing and demonstration purposes, let’s work with a smaller batch than the ones created when shaping our data.\n\nbatch = X_train[:4]\nbatch.shape\n\ntorch.Size([4, 784])\n\n\n\npreds = linear1(batch)\npreds\n\ntensor([[ -0.5287,  12.6047,  -9.0290,  -1.7505,   3.7686,  18.8489,  -1.8141,\n         -12.2232,  -5.1421,  -6.6316],\n        [ 18.8942,  10.7898,   9.2573,   7.9989,  -1.2884,  19.0238,  -5.8788,\n           6.5045, -10.2431,   5.5865],\n        [  6.3639,  14.0687,   0.7705,  -1.3580,   1.4220,   7.3108,  -7.4359,\n          -6.8101,  -5.9212,  23.7016],\n        [-14.7847,   3.0711,  -0.6092,   2.2720,  -1.1361,   3.7617,   5.1197,\n           5.3868,  -1.5228,  -7.6523]], grad_fn=<AddBackward0>)\n\n\n\nloss = mnist_loss(preds, y_train[:4])\nloss\n\ntensor(5.9773, grad_fn=<DivBackward1>)\n\n\nNow we can calculate the gradients:\n\nloss.backward()\nweights.grad.shape,weights.grad.mean(),bias.grad\n\n(torch.Size([784, 10]), tensor(2.4328e-10), tensor([5.9605e-08]))\n\n\nThe following function combines the above code and generalizes to models other than linear1.\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\n\ncalc_grad(batch, y_train[:4], linear1)\nweights.grad.mean(),bias.grad\n\n(tensor(4.8657e-10), tensor([1.1921e-07]))\n\n\nUsing the calculated gradients, we can update the weights for each epoch. We need to specify a learning rate and reset the gradients to 0, since loss.backward actually adds the gradients of loss to any gradients that are currently stored.\nlr = 1.\n\nfor p in weights,bias:\n    p.data -= p.grad*lr\n    p.grad.zero_()\nFinally, we can define a function that trains the model for one epoch:\n\ndef train_epoch(model, params, lr=1):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\nWe also probably want to check the accuracy of our model. The label that the model predicts is the label with the highest activation:\n\ndef batch_accuracy(xb, yb):\n    label = torch.argmax(xb, dim=1)\n    y_truth = torch.argmax(yb, dim=1)\n    correct = y_truth == label\n    return correct.float().mean()\n\n\nbatch_accuracy(linear1(batch), y_train[:4])\n\ntensor(0.)\n\n\nTo get the accuracy for the whole epoch, we must call batch_accuracy with batches of the validation dataset, then take the mean over all batches.\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\nFinally, we can see if our code works by checking if the accuracy improves!\n\nparams = weights, bias\nfor i in range(40):\n    train_epoch(linear1, params)\n    print(validate_epoch(linear1), end=' ')\n\n0.8213 0.8532 0.8661 0.8736 0.8769 0.8808 0.8848 0.8869 0.8895 0.8914 0.8929 0.894 0.8949 0.8964 0.8973 0.8977 0.8978 0.8979 0.8981 0.8986 0.8998 0.9 0.9004 0.9016 0.9017 0.9027 0.9027 0.9032 0.9037 0.9047 0.9053 0.9058 0.9061 0.9062 0.9061 0.9062 0.906 0.9061 0.906 0.9063"
  },
  {
    "objectID": "posts/digit-classifier/index.html#simplifying-code",
    "href": "posts/digit-classifier/index.html#simplifying-code",
    "title": "MNIST Digit Classifier",
    "section": "Simplifying Code",
    "text": "Simplifying Code\nnn.Linear does the same thing as our init_params and linear1 functions together. Also, fastai’s SGD class provides us with functions that takes care of updating the parameters and reseting the gradients of our model. By replacing some code, we can boil the training portion of our MNIST classifer down to the following:\n\ndef mnist_loss(xb, yb):\n    loss = nn.CrossEntropyLoss()\n    return loss(xb, yb)\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\ndef train_epoch_simple(model):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n        \ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch_simple(model)\n        print(validate_epoch(model), end=' ')\n        \nlinear_model = nn.Linear(28*28,10)\nopt = SGD(linear_model.parameters(), lr=1)\ntrain_model(linear_model, 20)\n\n0.8983 0.9057 0.9092 0.9113 0.9143 0.9149 0.9154 0.9154 0.9162 0.9161 0.9166 0.9167 0.9166 0.9166 0.9164 0.9169 0.917 0.9173 0.9173 0.9175 \n\n\nFast.ai provides us with Learner.fit, which we can use instead of train_model to significantly reduce the amount of code we need to write. To use the function, we must create a Learner, which requires a DataLoaders of our training and validation datasets:\n\ndls = DataLoaders(dl, valid_dl)\n\nThen, we pass in DataLoaders, the model, the optimization function, the loss function, and optionally any metrics to print into the Learner constructor to create one:\n\nlearn = Learner(dls, nn.Linear(28*28,10), opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\nFinally, we can call Learner.fit:\n\nlearn.fit(10, lr=1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      batch_accuracy\n      time\n    \n  \n  \n    \n      0\n      0.413918\n      0.365917\n      0.897400\n      00:00\n    \n    \n      1\n      0.326721\n      0.337396\n      0.905100\n      00:00\n    \n    \n      2\n      0.302027\n      0.325679\n      0.908500\n      00:00\n    \n    \n      3\n      0.289524\n      0.319092\n      0.910000\n      00:00\n    \n    \n      4\n      0.281248\n      0.314855\n      0.912800\n      00:00\n    \n    \n      5\n      0.275084\n      0.311911\n      0.913200\n      00:00\n    \n    \n      6\n      0.270192\n      0.309764\n      0.913500\n      00:00\n    \n    \n      7\n      0.266153\n      0.308146\n      0.913900\n      00:00\n    \n    \n      8\n      0.262723\n      0.306901\n      0.914300\n      00:00\n    \n    \n      9\n      0.259748\n      0.305928\n      0.914900\n      00:00"
  },
  {
    "objectID": "posts/digit-classifier/index.html#adding-non-linearity",
    "href": "posts/digit-classifier/index.html#adding-non-linearity",
    "title": "MNIST Digit Classifier",
    "section": "Adding Non-linearity",
    "text": "Adding Non-linearity\nTo expand upon our model, we can add another layer on top of what we have now. However, mathematically speaking, the composition of two linear functions is another linear function. Therefore, stacking two linear classifiers on top of each other is equivalent to having just one linear classifier.\nTherefore, we must add some non-linearity between linear layers. We often do this by through activation functions; a common one is the ReLU function:\n\nx = np.linspace(-5,5,100)\ny = np.maximum(0, x)\n\nplt.plot(x, y)\n\n\n\n\nnn.Sequential creates a module that will call each of the listed layers or functions.\nOur first layer takes in 784 inputs (pixels) and outputs 60 numbers. Those 60 numbers are then each passed into the ReLU function before going into the second layer. The second layer has 10 outputs, which as before, is the probability of each digit being the lable.\n\nsimple_net = nn.Sequential(\n    nn.Linear(784,100),\n    nn.ReLU(),\n    nn.Linear(100,10)\n)\n\nWe can train this model using Learner.fit as well (we are using more epochs and smaller learning rate, since it is a larger model):\n\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\nlearn.fit(60, lr=0.1)\n\nThe output is ommitted to save room; the training process is recorded in learn.recorder, with the table of output stored in the values attribute, so we can plot the accuracy over training as:\n\nplt.plot(L(learn.recorder.values).itemgot(2))\n\n\n\n\n\n# final accuracy\nlearn.recorder.values[-1][2]\n\n0.9652000069618225"
  },
  {
    "objectID": "posts/digit-classifier/index.html#making-a-submission",
    "href": "posts/digit-classifier/index.html#making-a-submission",
    "title": "MNIST Digit Classifier",
    "section": "Making a Submission",
    "text": "Making a Submission\nThough our very basic model is far from perfect, we can still submit it to the competition! Recall that we stored the test.csv data into the df_test DataFrame. We need to first normalize the pixels then plug it into our model:\n\nX_test = tensor(df_test)/ 255.0\n\n\nx_val = simple_net(X_test)\ny_pred = torch.argmax(x_val, dim=1)\ny_pred = y_pred.tolist()\n\nFinally, we can create a submission file in our current directory:\n\nwith open(\"submission.csv\", 'w+') as f :\n    f.write('ImageId,Label\\n')\n    for i in range(len(y_pred)) :\n        f.write(\"\".join([str(i+1),',',str(y_pred[i]),'\\n']))\n\nThen submit to Kaggle!\n\n!kaggle competitions submit -c digit-recognizer -f submission.csv -m \"First Attempt\"\n\n100%|█████████████████████████████████████████| 208k/208k [00:01<00:00, 165kB/s]\nSuccessfully submitted to Digit Recognizer"
  },
  {
    "objectID": "index.html#ls-posts",
    "href": "index.html#ls-posts",
    "title": "seong/bin/blog",
    "section": "$ ls posts",
    "text": "$ ls posts"
  },
  {
    "objectID": "posts/gradient-descent/index.html",
    "href": "posts/gradient-descent/index.html",
    "title": "Gradient Descent Explained",
    "section": "",
    "text": "This article will how and why gradient descent works; to follow along, you should know what a gradient is (duh). If you don’t, give this article a read.\n\nHow gradient descent works\nGradient descent can be summarized into the following steps:\n\nChoose initial values of parameters \\((\\theta = \\{\\theta_0, \\theta_1, \\dots, \\theta_d\\} \\in \\mathbb{R}^{d+1})\\)\nStep into the opposite direction of the gradient of the loss function by a factor of the learning rate: \\[\\theta_j \\leftarrow\\theta_j-\\alpha \\frac{\\partial J(\\theta)}{\\partial\\theta_j}\\qquad \\forall j =0\\dots d \\quad \\text{simultaneously}\\]\nRepeat unitl convergence (when the Euclidian norm between updated parameters converges: \\(\\|\\theta_\\text{new}-\\theta_\\text{old}\\|<\\epsilon\\) for some \\(\\epsilon\\))\n\nA lot of notation; kinda confusing. In human language, here is what happens:\n\nStart off with a set of parameters.\nRepeat the following steps until the parameters don’t change much:\n\nCalculate the gradient of the loss function with respect to each parameter\nMove the parameters in the opposite direction of the gradient by a factor of the learning rate\n\n\n\nNote: The learning rate \\(\\alpha\\) is a hyperparameter that controls how big the steps are. I will cover it in following sections, but to learn about hyperparameters in general, click here.\n\nIt almost seems magical that this simple procedure can find the minimum of a loss function. To figure out why this works, we need to figure out what exactly moving in the opposite direction of the gradient means.\n\n\nWhy gradient descent works\nImagine you are a hiker on a mountain that wants to find the lowest point. To get to the lowest point as quickly as possible, you probably want to walk in the direction of steepest descent.\nHowever, since a mountain is irregular, the direction of steepest descent is not always the same. Therefore, you will have to take a small step, recalculate the direction, take another small step, and so on. Eventually, you will find the lowest point.\n\nThis is exactly what gradient descent does. It takes small steps in the direction of steepest descent until the cost function is minimized.\nIn other words, moving in the opposite direction of the gradient is equivalent to taking a step in the direction of steepest descent. This can be proven if we prove that the gradient is the direction of steepest ascent. In the following section, I will present a proof that works for any dimension; feel free to skip it if you are not interested.\n\nProof: The gradient is the direction of steepest descent\nSuppose \\(f(x,y)\\) is differentiable at \\((x,y)\\), \\(\\nabla f(x,y) \\ne \\vec{0}\\), and \\(\\vec{u}\\) is a unit vector. The directional derivative of \\(f\\) in the direction of \\(\\vec{u}\\) can be calculated as follows: \\[{D_{\\vec u}}f\\left( {x,y} \\right) = \\nabla f \\cdot \\vec{u}\\]\nWe can decompose the dot product into the following: \\[\\begin{align}\n{D_{\\vec u}}f\\left( {x,y} \\right) &= \\nabla f \\cdot \\vec{u} \\\\ &= \\|\\nabla f\\|\\|\\vec{u}\\|\\cos{\\theta} \\\\ &= \\|\\nabla f\\|\\cos{\\theta}.\n\\end{align}\\] Since \\(-1 \\le \\cos{\\theta} \\le 1\\), the directional derivative, or the rate of change of \\(f\\), is maximum when \\(\\theta = 0\\), which is when it points in the same direction of \\(\\nabla f. \\qquad \\square\\)\n\n\n\nLearning rate\nAs mentioned earlier, \\(\\alpha\\) is the learning rate, a hyperparameter that controls how big the steps are. To see why it matters, let’s go back to the hiking analogy.\nIf the hiker takes too many steps in one direction before recalculating the gradient, he or she might continuously overshoot the lowest point. If the hiker takes too few steps, he or she might never reach the lowest point in time.\n\nLikewise, if the learning rate is too big, the parameters might be adjusted too much, missing the minimum of the loss function. If the learning rate is too small, the parameters will be adjusted too little, and the algorithm will take too long to converge.\n\n\nTypes of Gradient Descent\nThere are two types of gradient descent: batch gradient descent and stochastic gradient descent. Then there is a hybrid of the two called mini-batch gradient descent.\n\nTo make the explanation easier, I will use the following notation:\n\nGiven that there are \\(n\\) examples in our training data, let \\(\\ell(x^{(i)}, y^{(i)}, \\theta)\\) be the loss of one example \\((x^{(i)}, y^{(i)})\\) of the training data. Then, \\[J(\\theta) = \\frac{1}{n}\\sum^n_{i=1}\\ell(x^{(i)}, y^{(i)}, \\theta).\\]\n\n\nBatch Gradient Descent\nBatch gradient descent is the most straightforward way to implement gradient descent. It averages the gradient over entire (or “batches” of the) dataset: \\[\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta J(\\theta)\\]\nHere are some pros and cons of batch gradient descent:\nPros: - Guaranteed to converge to the global minimum (given enough time) - Easy to implement\nCons: - Slow to converge - Requires a lot of memory\n\n\nStochastic Gradient Descent\nStochastic gradient descent (SGD) is the opposite of batch gradient descent. Instead of averaging the gradient over the entire dataset, it updates the gradient for every example (iterate over \\(i = 1, 2, \\dots, n\\)): \\[\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta \\,\\ell(x^{(i)}, y^{(i)}, \\theta)\\]\n\n\nComparison of Batch and Stochastic Gradient Descent\n\n\n\n\n\n\n\n\n\n\nBatch\nStochastic\n\n\n\n\n\nPros\n- stable convergence and error  - exploits hardware optimized for matrix computations  - more direct path is taken towards the minimum\n- scalable to large datasets  - memory efficient  - computationally cheap to calculate gradient  - implicit regularization\n\n\n\nCons\n- computationally expensive to calculate gradient - memory intensive\n- high noise in gradient  - many updates before convergence  - cannot exploit optimized matrix operations\n\n\n\n\n\n\nMini-batch Gradient Descent\nMBGD is a compromise between the above two variations. It has the advantages of both stochastic and batch gradient descent, hence is used most often in practice. It samples a batch of \\(B\\) points \\(D_B\\) at random from the full dataset \\(D\\) without replacement: \\[\\theta \\leftarrow \\theta - \\alpha \\frac{1}{B}\\sum_{\\left(x^{(i)},y^{(i)}\\right)\\in D_B} \\nabla_\\theta \\,\\ell(x^{(i)}, y^{(i)}, \\theta)\\] When \\(B=1,\\) MBGD is just stochastic gradient descent; when \\(B=n\\), it is batch gradient descent.\n\n\n\nClosing Thoughts\nGradient descent is a very powerful technique that is used in many machine learning algorithms. I hope this post has helped you understand it better. If you have any questions or comments, please leave them below. Thanks for reading!"
  },
  {
    "objectID": "posts/gradient-descent/index.html#procedure",
    "href": "posts/gradient-descent/index.html#procedure",
    "title": "Gradient Descent Explained",
    "section": "Procedure",
    "text": "Procedure\n\nChoose initial values of parameters \\((\\theta = \\{\\theta_0, \\theta_1, \\dots, \\theta_d\\} \\in \\mathbb{R}^{d+1})\\)\nStep into the opposite direction of the [[gradient]] by a factor of the [[learning rate]] \\[\\theta_j \\leftarrow\\theta_j-\\alpha \\frac{\\partial J(\\theta)}{\\partial\\theta_j}\\qquad \\forall j =0\\dots d \\quad \\text{simultaneously}\\]\nRepeat unitl convergence (when the Euclidian [[norm]] between updated parameters converges: \\(\\|\\theta_\\text{new}-\\theta_\\text{old}\\|<\\epsilon\\) for some \\(\\epsilon\\))\n\nWe update the parameters in the opposite direction of the gradient because the [[gradient points to steepest ascent]]."
  },
  {
    "objectID": "posts/gradient-descent/index.html#online-learning-vs.-batch-learning",
    "href": "posts/gradient-descent/index.html#online-learning-vs.-batch-learning",
    "title": "Gradient Descent Explained",
    "section": "online learning vs. batch learning",
    "text": "online learning vs. batch learning\n[zzgd comp.png] Let \\(\\ell(x^{(i)}, y^{(i)}, \\theta)\\) be the loss of one example \\((x^{(i)}, y^{(i)})\\) of the training data. Then, \\[J(\\theta) = \\frac{1}{n}\\sum^n_{i=1}\\ell(x^{(i)}, y^{(i)}, \\theta).\\]\n\nBatch Gradient Descent\nAverages gradient over entire (or batches of) dataset: \\[\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta J(\\theta)\\]\n\n\nStochastic Gradient Descent\nUpdate gradient for every example (iterate over \\(i = 1, 2, \\dots, n\\)): \\[\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta \\,\\ell(x^{(i)}, y^{(i)}, \\theta)\\]\n\nAdvantages\n\nscalability to large datasets\nmemory efficient: single example is processed at a time\ncomputationally cheap: single example needed to compute gradient\nimplicit regularization\n\n\n\nDisadvantages\n\nhigh noise in gradient, which can lead to many updates before convergence\ncannot exploit modern hardware optimized for [[matrix]]/[[vector]] computations\n\n\n\n\nMini-batch Gradient Descent\nMBGD is a compromise between the above two variations. It samples a batch of \\(B\\) points \\(D_B\\) at random from the full dataset \\(D\\) without replacement: \\[\\theta \\leftarrow \\theta - \\alpha \\frac{1}{B}\\sum_{\\left(x^{(i)},y^{(i)}\\right)\\in D_B} \\nabla_\\theta \\,\\ell(x^{(i)}, y^{(i)}, \\theta)\\] When \\(B=1,\\) MBGD is just stochastic gradient descent; when \\(B=n\\), it is batch gradient descent."
  },
  {
    "objectID": "posts/gradient-descent/index.html#links",
    "href": "posts/gradient-descent/index.html#links",
    "title": "Gradient Descent Explained",
    "section": "Links",
    "text": "Links\n\n[[convergence and divergence (sequences)]]\n[[partial derivative]]"
  },
  {
    "objectID": "posts/gradient-descent/index.html#how-gradient-descent-works",
    "href": "posts/gradient-descent/index.html#how-gradient-descent-works",
    "title": "Gradient Descent Explained",
    "section": "How gradient descent works",
    "text": "How gradient descent works\nGradient descent can be summarized into the following steps:\n\nChoose initial values of parameters \\((\\theta = \\{\\theta_0, \\theta_1, \\dots, \\theta_d\\} \\in \\mathbb{R}^{d+1})\\)\nStep into the opposite direction of the gradient of the loss function by a factor of the learning rate: \\[\\theta_j \\leftarrow\\theta_j-\\alpha \\frac{\\partial J(\\theta)}{\\partial\\theta_j}\\qquad \\forall j =0\\dots d \\quad \\text{simultaneously}\\]\nRepeat unitl convergence (when the Euclidian norm between updated parameters converges: \\(\\|\\theta_\\text{new}-\\theta_\\text{old}\\|<\\epsilon\\) for some \\(\\epsilon\\))\n\nA lot of notation; kinda confusing. In human language, here is what happens:\n\nStart off with a set of parameters.\nRepeat the following steps until the parameters don’t change much:\n\nCalculate the gradient of the loss function with respect to each parameter\nMove the parameters in the opposite direction of the gradient by a factor of the learning rate\n\n\n\nNote: The learning rate \\(\\alpha\\) is a hyperparameter that controls how big the steps are. I will cover it in following sections, but to learn about hyperparameters in general, click here.\n\nIt almost seems magical that this simple procedure can find the minimum of a loss function. To figure out why this works, we need to figure out what exactly moving in the opposite direction of the gradient means."
  },
  {
    "objectID": "posts/gradient-descent/index.html#why-gradient-descent-works",
    "href": "posts/gradient-descent/index.html#why-gradient-descent-works",
    "title": "Gradient Descent Explained",
    "section": "Why gradient descent works",
    "text": "Why gradient descent works\nImagine you are a hiker on a mountain that wants to find the lowest point. To get to the lowest point as quickly as possible, you probably want to walk in the direction of steepest descent.\nHowever, since a mountain is irregular, the direction of steepest descent is not always the same. Therefore, you will have to take a small step, recalculate the direction, take another small step, and so on. Eventually, you will find the lowest point.\n\nThis is exactly what gradient descent does. It takes small steps in the direction of steepest descent until the cost function is minimized.\nIn other words, moving in the opposite direction of the gradient is equivalent to taking a step in the direction of steepest descent. This can be proven if we prove that the gradient is the direction of steepest ascent. In the following section, I will present a proof that works for any dimension; feel free to skip it if you are not interested.\n\nProof: The gradient is the direction of steepest descent\nSuppose \\(f(x,y)\\) is differentiable at \\((x,y)\\), \\(\\nabla f(x,y) \\ne \\vec{0}\\), and \\(\\vec{u}\\) is a unit vector. The directional derivative of \\(f\\) in the direction of \\(\\vec{u}\\) can be calculated as follows: \\[{D_{\\vec u}}f\\left( {x,y} \\right) = \\nabla f \\cdot \\vec{u}\\]\nWe can decompose the dot product into the following: \\[\\begin{align}\n{D_{\\vec u}}f\\left( {x,y} \\right) &= \\nabla f \\cdot \\vec{u} \\\\ &= \\|\\nabla f\\|\\|\\vec{u}\\|\\cos{\\theta} \\\\ &= \\|\\nabla f\\|\\cos{\\theta}.\n\\end{align}\\] Since \\(-1 \\le \\cos{\\theta} \\le 1\\), the directional derivative, or the rate of change of \\(f\\), is maximum when \\(\\theta = 0\\), which is when it points in the same direction of \\(\\nabla f. \\qquad \\square\\)"
  },
  {
    "objectID": "posts/gradient-descent/index.html#learning-rate",
    "href": "posts/gradient-descent/index.html#learning-rate",
    "title": "Gradient Descent Explained",
    "section": "Learning rate",
    "text": "Learning rate\nAs mentioned earlier, \\(\\alpha\\) is the learning rate, a hyperparameter that controls how big the steps are. To see why it matters, let’s go back to the hiking analogy.\nIf the hiker takes too many steps in one direction before recalculating the gradient, he or she might continuously overshoot the lowest point. If the hiker takes too few steps, he or she might never reach the lowest point in time.\n\nLikewise, if the learning rate is too big, the parameters might be adjusted too much, missing the minimum of the loss function. If the learning rate is too small, the parameters will be adjusted too little, and the algorithm will take too long to converge."
  },
  {
    "objectID": "posts/gradient-descent/index.html#types-of-gradient-descent",
    "href": "posts/gradient-descent/index.html#types-of-gradient-descent",
    "title": "Gradient Descent Explained",
    "section": "Types of Gradient Descent",
    "text": "Types of Gradient Descent\nThere are two types of gradient descent: batch gradient descent and stochastic gradient descent. Then there is a hybrid of the two called mini-batch gradient descent.\n\nTo make the explanation easier, I will use the following notation:\n\nGiven that there are \\(n\\) examples in our training data, let \\(\\ell(x^{(i)}, y^{(i)}, \\theta)\\) be the loss of one example \\((x^{(i)}, y^{(i)})\\) of the training data. Then, \\[J(\\theta) = \\frac{1}{n}\\sum^n_{i=1}\\ell(x^{(i)}, y^{(i)}, \\theta).\\]\n\n\nBatch Gradient Descent\nBatch gradient descent is the most straightforward way to implement gradient descent. It averages the gradient over entire (or “batches” of the) dataset: \\[\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta J(\\theta)\\]\nHere are some pros and cons of batch gradient descent:\nPros: - Guaranteed to converge to the global minimum (given enough time) - Easy to implement\nCons: - Slow to converge - Requires a lot of memory\n\n\nStochastic Gradient Descent\nStochastic gradient descent (SGD) is the opposite of batch gradient descent. Instead of averaging the gradient over the entire dataset, it updates the gradient for every example (iterate over \\(i = 1, 2, \\dots, n\\)): \\[\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta \\,\\ell(x^{(i)}, y^{(i)}, \\theta)\\]\n\n\nComparison of Batch and Stochastic Gradient Descent\n\n\n\n\n\n\n\n\n\n\nBatch\nStochastic\n\n\n\n\n\nPros\n- stable convergence and error  - exploits hardware optimized for matrix computations  - more direct path is taken towards the minimum\n- scalable to large datasets  - memory efficient  - computationally cheap to calculate gradient  - implicit regularization\n\n\n\nCons\n- computationally expensive to calculate gradient - memory intensive\n- high noise in gradient  - many updates before convergence  - cannot exploit optimized matrix operations\n\n\n\n\n\n\nMini-batch Gradient Descent\nMBGD is a compromise between the above two variations. It has the advantages of both stochastic and batch gradient descent, hence is used most often in practice. It samples a batch of \\(B\\) points \\(D_B\\) at random from the full dataset \\(D\\) without replacement: \\[\\theta \\leftarrow \\theta - \\alpha \\frac{1}{B}\\sum_{\\left(x^{(i)},y^{(i)}\\right)\\in D_B} \\nabla_\\theta \\,\\ell(x^{(i)}, y^{(i)}, \\theta)\\] When \\(B=1,\\) MBGD is just stochastic gradient descent; when \\(B=n\\), it is batch gradient descent."
  },
  {
    "objectID": "posts/gradient-descent/index.html#closing-thoughts",
    "href": "posts/gradient-descent/index.html#closing-thoughts",
    "title": "Gradient Descent Explained",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nGradient descent is a very powerful technique that is used in many machine learning algorithms. I hope this post has helped you understand it better. If you have any questions or comments, please leave them below. Thanks for reading!"
  }
]