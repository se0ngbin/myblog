<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>seong/bin/blog</title>
<link>https://blog.seongbin.me/index.html</link>
<atom:link href="https://blog.seongbin.me/index.xml" rel="self" type="application/rss+xml"/>
<description>A journey through computer science</description>
<generator>quarto-1.1.251</generator>
<lastBuildDate>Thu, 01 Sep 2022 07:00:00 GMT</lastBuildDate>
<item>
  <title>Reddit Depression Predictor Using Hugging Face Transformers</title>
  <dc:creator>Seongbin Park</dc:creator>
  <link>https://blog.seongbin.me/posts/depression-predictor/index.html</link>
  <description><![CDATA[ 




<p>This post will cover how to fine tune a NLP classification model using <a href="https://huggingface.co/">Hugging Face</a> <a href="https://huggingface.co/docs/transformers/index">Transformers</a>. I will be using the <a href="https://www.kaggle.com/datasets/infamouscoder/depression-reddit-cleaned">cleaned reddit depression dataset</a>, which specifies whether or not a post was made in the <a href="https://www.reddit.com/r/depression/">r/depression</a> subreddit, to train my model.</p>
<p>The final model will be able to classify whether or not a block of text was written in the r/depression subreddit with 98% accuracy. I will create a demo to have users input text and see if it is shows signs of depression.</p>
<p>The similarity of a block of text to posts in r/depression is not perfectly correlated to the text showing signs of clinical depression, so the accuracy of our demo cannot be quantified. However, it can still provide some insight into what type of texts <em>might</em> have been written by depressed patients.</p>
<p>Credits go to the hugging face documentation as well as <a href="https://fast.ai">fast.ai</a>, which are both great educational resources.</p>
<section id="obtaining-data" class="level2">
<h2 class="anchored" data-anchor-id="obtaining-data">Obtaining Data</h2>
<p>First, I will fetch the dataset using <code>opendatasets</code>:</p>
<div class="cell" data-outputid="4c7ac52c-32d4-48ea-9604-e57de48a9016" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb1-2">    <span class="im" style="color: #00769E;">import</span> opendatasets <span class="im" style="color: #00769E;">as</span> od</span>
<span id="cb1-3"><span class="cf" style="color: #003B4F;">except</span>:</span>
<span id="cb1-4">    <span class="op" style="color: #5E5E5E;">!</span>pip install opendatasets</span>
<span id="cb1-5">    <span class="im" style="color: #00769E;">import</span> opendatasets <span class="im" style="color: #00769E;">as</span> od</span></code></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">from</span> pathlib <span class="im" style="color: #00769E;">import</span> Path</span>
<span id="cb2-2"></span>
<span id="cb2-3">path <span class="op" style="color: #5E5E5E;">=</span> Path(<span class="st" style="color: #20794D;">"depression-reddit-cleaned"</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> path.exists():</span>
<span id="cb3-2">    od.download(<span class="st" style="color: #20794D;">"https://www.kaggle.com/datasets/infamouscoder/depression-reddit-cleaned"</span>)</span></code></pre></div>
</div>
</section>
<section id="brief-eda" class="level1">
<h1>Brief EDA</h1>
<p>Now that the dataset is imported, we can create a <code>Dataframe</code>:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd</span>
<span id="cb4-2"></span>
<span id="cb4-3">df <span class="op" style="color: #5E5E5E;">=</span> pd.read_csv(path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'depression_dataset_reddit_cleaned.csv'</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>clean_text</th>
      <th>is_depression</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>we understand that most people who reply immed...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>welcome to r depression s check in post a plac...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>anyone else instead of sleeping more when depr...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>i ve kind of stuffed around a lot in my life d...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>sleep is my greatest and most comforting escap...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>7726</th>
      <td>is that snow</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7727</th>
      <td>moulin rouge mad me cry once again</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7728</th>
      <td>trying to shout but can t find people on the list</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7729</th>
      <td>ughh can t find my red sox hat got ta wear thi...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7730</th>
      <td>slept wonderfully finally tried swatching for ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>7731 rows × 2 columns</p>
</div>
</div>
</div>
<p>It seems like we have 7731 examples in our dataset and 2 columns: <code>clean_text</code> and <code>is_depression</code>.</p>
<div class="cell" data-outputid="ad94e954-348e-4972-fd0c-fb02b7085f8c" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">df.count()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>clean_text       7731
is_depression    7731
dtype: int64</code></pre>
</div>
</div>
<p>Since this is a cleaned dataset, there are no null values or weird labels:</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">df.isnull().<span class="bu" style="color: null;">sum</span>()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>clean_text       0
is_depression    0
dtype: int64</code></pre>
</div>
</div>
<div class="cell" data-outputid="f45df576-31a1-4f1c-bbd7-b0dc907abb1b" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">df[<span class="st" style="color: #20794D;">'is_depression'</span>].unique()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>array([1, 0])</code></pre>
</div>
</div>
<p>Let’s take a look at a text block:</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">df[<span class="st" style="color: #20794D;">'clean_text'</span>][<span class="dv" style="color: #AD0000;">2</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>'anyone else instead of sleeping more when depressed stay up all night to avoid the next day from coming sooner may be the social anxiety in me but life is so much more peaceful when everyone else is asleep and not expecting thing of you'</code></pre>
</div>
</div>
<p>Since neural networks expects numbers, not sentences, as inputs, we must somehow convert text blocks into a sequence of numbers. Therefore, each text block is first split up up into <em>tokens</em> (through tokenization), which are then converted to numbers (through numericalization).</p>
</section>
<section id="tokenization" class="level1">
<h1>Tokenization</h1>
<p>Before we tokenize our data, we need to convert our <code>Dataframe</code> into a <code>Dataset</code>.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="im" style="color: #00769E;">from</span> datasets <span class="im" style="color: #00769E;">import</span> Dataset</span>
<span id="cb14-2"></span>
<span id="cb14-3">ds <span class="op" style="color: #5E5E5E;">=</span> Dataset.from_pandas(df)</span>
<span id="cb14-4">ds</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>Dataset({
    features: ['clean_text', 'is_depression'],
    num_rows: 7731
})</code></pre>
</div>
</div>
<p>This is for later, but Hugging Face Transformers always assumes that your labels has the column name <code>labels</code>. In our dataset it’s currently <code>score</code>, so we should to rename it:</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">ds <span class="op" style="color: #5E5E5E;">=</span> ds.rename_columns({<span class="st" style="color: #20794D;">'is_depression'</span>:<span class="st" style="color: #20794D;">'labels'</span>})</span>
<span id="cb16-2">ds</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>Dataset({
    features: ['clean_text', 'labels'],
    num_rows: 7731
})</code></pre>
</div>
</div>
<p>To import a tokenizer, we need to use <code>AutoTokenizer</code>:</p>
<div class="cell" data-outputid="5263e068-ab2f-41a8-d4b5-e0584658b5a8" data-execution_count="9">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> AutoTokenizer</span></code></pre></div>
</div>
<p>I will use the <a href="https://huggingface.co/distilbert-base-uncased">DistilBERT base model</a>, which, as the name suggests, is a <a href="https://arxiv.org/abs/1910.01108">distiled</a> version of the <a href="https://huggingface.co/bert-base-uncased">BERT base model</a>.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">model_name <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"distilbert-base-uncased"</span></span></code></pre></div>
</div>
<p>We will use <code>from_pretrained</code> to instantiate a tokenizer class from a pretrained model vocabulary. The tokenizer class to instantiate is selected based on the model (“distilbert-base-uncased” in our case).</p>
<div class="cell" data-outputid="a5cba05d-be80-4b36-aeee-138242c07ded" data-execution_count="11">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">tokenizer <span class="op" style="color: #5E5E5E;">=</span> AutoTokenizer.from_pretrained(model_name)</span></code></pre></div>
</div>
<p>Creating a preprocessing function to tokenize text and truncate sequences:</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="kw" style="color: #003B4F;">def</span> tokenize_function(examples):</span>
<span id="cb21-2">    <span class="cf" style="color: #003B4F;">return</span> tokenizer(examples[<span class="st" style="color: #20794D;">"clean_text"</span>], truncation<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<p>Using the function above and the Datasets map function, we can apply the preprocessing function over the entire dataset. You can speed up the map function by setting batched=True to process multiple elements of the dataset at once:</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">tokenized_ds <span class="op" style="color: #5E5E5E;">=</span> ds.<span class="bu" style="color: null;">map</span>(tokenize_function, batched<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c371e34340974b3fb06dbc77c4383962","version_major":2,"version_minor":0}
</script>
</div>
</div>
<p>While it is possible to pad your text in the tokenizer function by setting <code>padding=True</code>, dynamic padding is more efficient. <code>data_collator</code> will be used later for this purpose.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> DataCollatorWithPadding</span>
<span id="cb23-2"></span>
<span id="cb23-3">data_collator <span class="op" style="color: #5E5E5E;">=</span> DataCollatorWithPadding(tokenizer<span class="op" style="color: #5E5E5E;">=</span>tokenizer)</span></code></pre></div>
</div>
</section>
<section id="creating-validation-and-test-sets" class="level1">
<h1>Creating Validation and Test Sets</h1>
<p>We can easily split our dataset into training and validation sets using <code>train_test_split</code>:</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">dds <span class="op" style="color: #5E5E5E;">=</span> tokenized_ds.train_test_split(<span class="fl" style="color: #AD0000;">0.25</span>, seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">42</span>)</span>
<span id="cb24-2">dds</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>DatasetDict({
    train: Dataset({
        features: ['clean_text', 'labels', 'input_ids', 'attention_mask'],
        num_rows: 5798
    })
    test: Dataset({
        features: ['clean_text', 'labels', 'input_ids', 'attention_mask'],
        num_rows: 1933
    })
})</code></pre>
</div>
</div>
</section>
<section id="train" class="level1">
<h1>Train</h1>
<p>I will turn off the warnings returned by Hugging Face for readability:</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><span class="im" style="color: #00769E;">import</span> warnings, logging</span>
<span id="cb26-2"></span>
<span id="cb26-3">warnings.simplefilter(<span class="st" style="color: #20794D;">'ignore'</span>)</span>
<span id="cb26-4">logging.disable(logging.WARNING)</span></code></pre></div>
</div>
<p>Similarly to how we instantiated our tokenizer, we will instantiate our model using <code>from_pretrained</code>.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> AutoModelForSequenceClassification, TrainingArguments, Trainer</span>
<span id="cb27-2"></span>
<span id="cb27-3">model <span class="op" style="color: #5E5E5E;">=</span> AutoModelForSequenceClassification.from_pretrained(model_name, num_labels<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
</div>
<p>To evaluate our model’s performance, we will use accuracy:</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><span class="im" style="color: #00769E;">from</span> datasets <span class="im" style="color: #00769E;">import</span> load_metric</span>
<span id="cb28-2"></span>
<span id="cb28-3">metric <span class="op" style="color: #5E5E5E;">=</span> load_metric(<span class="st" style="color: #20794D;">"accuracy"</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb29-2"></span>
<span id="cb29-3"><span class="kw" style="color: #003B4F;">def</span> compute_metrics(eval_pred):</span>
<span id="cb29-4">    logits, labels <span class="op" style="color: #5E5E5E;">=</span> eval_pred</span>
<span id="cb29-5">    predictions <span class="op" style="color: #5E5E5E;">=</span> np.argmax(logits, axis<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb29-6">    <span class="cf" style="color: #003B4F;">return</span> metric.compute(predictions<span class="op" style="color: #5E5E5E;">=</span>predictions, references<span class="op" style="color: #5E5E5E;">=</span>labels)</span></code></pre></div>
</div>
<p>We will be using the <a href="https://huggingface.co/docs/transformers/main_classes/trainer">Trainer</a> class, which provides an API for feature-complete training in PyTorch.</p>
<p>Before instantiating a <code>Trainer</code>, we need to create a <code>TrainingArguments</code> to access all the points of customization during training:</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">training_args <span class="op" style="color: #5E5E5E;">=</span> TrainingArguments(</span>
<span id="cb30-2">    output_dir<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"./results"</span>,</span>
<span id="cb30-3">    learning_rate<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">2e-5</span>,</span>
<span id="cb30-4">    per_device_train_batch_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>,</span>
<span id="cb30-5">    per_device_eval_batch_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>,</span>
<span id="cb30-6">    num_train_epochs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>,</span>
<span id="cb30-7">    weight_decay<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.01</span>,</span>
<span id="cb30-8">    evaluation_strategy<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"epoch"</span></span>
<span id="cb30-9">)</span></code></pre></div>
</div>
<p>Creating and training a <code>Trainer</code>:</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">trainer <span class="op" style="color: #5E5E5E;">=</span> Trainer(</span>
<span id="cb31-2">    model<span class="op" style="color: #5E5E5E;">=</span>model,</span>
<span id="cb31-3">    args<span class="op" style="color: #5E5E5E;">=</span>training_args,</span>
<span id="cb31-4">    train_dataset<span class="op" style="color: #5E5E5E;">=</span>dds[<span class="st" style="color: #20794D;">"train"</span>],</span>
<span id="cb31-5">    eval_dataset<span class="op" style="color: #5E5E5E;">=</span>dds[<span class="st" style="color: #20794D;">"test"</span>],</span>
<span id="cb31-6">    tokenizer<span class="op" style="color: #5E5E5E;">=</span>tokenizer,</span>
<span id="cb31-7">    data_collator<span class="op" style="color: #5E5E5E;">=</span>data_collator,</span>
<span id="cb31-8">    compute_metrics<span class="op" style="color: #5E5E5E;">=</span>compute_metrics,</span>
<span id="cb31-9">)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">trainer.train()</span></code></pre></div>
<div class="cell-output cell-output-display">


    <div>
      
      <progress value="726" max="726" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [726/726 02:26, Epoch 2/2]
    </div>
    <table class="dataframe table table-sm table-striped">
  <thead>
 <tr>
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>No log</td>
      <td>0.069008</td>
      <td>0.976203</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.126100</td>
      <td>0.073128</td>
      <td>0.981376</td>
    </tr>
  </tbody>
</table><p>
</p></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>TrainOutput(global_step=726, training_loss=0.09943613467466075, metrics={'train_runtime': 160.3661, 'train_samples_per_second': 72.31, 'train_steps_per_second': 4.527, 'total_flos': 1083160046271312.0, 'train_loss': 0.09943613467466075, 'epoch': 2.0})</code></pre>
</div>
</div>
<p>The accuracy is not bad, but I wanted to see if I could tweak the hyperparameters to improve the performance of our model. To make creating trainers easier, I defined a <code>get_trainer</code> function:</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><span class="kw" style="color: #003B4F;">def</span> get_trainer(model_name, data_collator<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">2e-5</span>, bs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>, epochs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>, train<span class="op" style="color: #5E5E5E;">=</span>dds[<span class="st" style="color: #20794D;">"train"</span>], test<span class="op" style="color: #5E5E5E;">=</span>dds[<span class="st" style="color: #20794D;">"test"</span>]):</span>
<span id="cb34-2">    </span>
<span id="cb34-3">    model <span class="op" style="color: #5E5E5E;">=</span> AutoModelForSequenceClassification.from_pretrained(model_name, num_labels<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb34-4">    </span>
<span id="cb34-5">    training_args <span class="op" style="color: #5E5E5E;">=</span> TrainingArguments(</span>
<span id="cb34-6">        output_dir<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"./results"</span>,</span>
<span id="cb34-7">        learning_rate<span class="op" style="color: #5E5E5E;">=</span>lr,</span>
<span id="cb34-8">        per_device_train_batch_size<span class="op" style="color: #5E5E5E;">=</span>bs,</span>
<span id="cb34-9">        per_device_eval_batch_size<span class="op" style="color: #5E5E5E;">=</span>bs,</span>
<span id="cb34-10">        num_train_epochs<span class="op" style="color: #5E5E5E;">=</span>epochs,</span>
<span id="cb34-11">        weight_decay<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.01</span>,</span>
<span id="cb34-12">        evaluation_strategy<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"epoch"</span>,</span>
<span id="cb34-13">        fp16<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>,</span>
<span id="cb34-14">    )</span>
<span id="cb34-15">    </span>
<span id="cb34-16">    <span class="cf" style="color: #003B4F;">return</span> Trainer(</span>
<span id="cb34-17">        model<span class="op" style="color: #5E5E5E;">=</span>model,</span>
<span id="cb34-18">        args<span class="op" style="color: #5E5E5E;">=</span>training_args,</span>
<span id="cb34-19">        train_dataset<span class="op" style="color: #5E5E5E;">=</span>train,</span>
<span id="cb34-20">        eval_dataset<span class="op" style="color: #5E5E5E;">=</span>test,</span>
<span id="cb34-21">        tokenizer<span class="op" style="color: #5E5E5E;">=</span>tokenizer,</span>
<span id="cb34-22">        data_collator<span class="op" style="color: #5E5E5E;">=</span>data_collator,</span>
<span id="cb34-23">        compute_metrics<span class="op" style="color: #5E5E5E;">=</span>compute_metrics,</span>
<span id="cb34-24">    )</span></code></pre></div>
</div>
<p>Since the whole dataset takes a while to train, I selected a smaller subset of the dataset for testing purposes:</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1">small_train <span class="op" style="color: #5E5E5E;">=</span> dds[<span class="st" style="color: #20794D;">"train"</span>].shuffle(seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">42</span>).select(<span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">1000</span>))</span>
<span id="cb35-2">small_eval <span class="op" style="color: #5E5E5E;">=</span> dds[<span class="st" style="color: #20794D;">"test"</span>].shuffle(seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">42</span>).select(<span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">1000</span>))</span></code></pre></div>
</div>
<p>In case our GPU runs out of memory, we can empty the cache:</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><span class="im" style="color: #00769E;">import</span> torch, gc</span>
<span id="cb36-2"></span>
<span id="cb36-3">gc.collect()</span>
<span id="cb36-4">torch.cuda.empty_cache()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">5</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">3</span>):</span>
<span id="cb37-2">    lr <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">10</span><span class="op" style="color: #5E5E5E;">**</span>i</span>
<span id="cb37-3">    trainer <span class="op" style="color: #5E5E5E;">=</span> get_trainer(model_name, lr<span class="op" style="color: #5E5E5E;">=</span>lr, bs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">32</span>, epochs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, train<span class="op" style="color: #5E5E5E;">=</span>small_train, test<span class="op" style="color: #5E5E5E;">=</span>small_eval)</span>
<span id="cb37-4">    <span class="bu" style="color: null;">print</span>(lr)</span>
<span id="cb37-5">    trainer.train()</span>
<span id="cb37-6">    gc.collect()</span>
<span id="cb37-7">    torch.cuda.empty_cache()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1e-05</code></pre>
</div>
<div class="cell-output cell-output-display">


    <div>
      
      <progress value="64" max="64" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [64/64 00:12, Epoch 2/2]
    </div>
    <table class="dataframe table table-sm table-striped">
  <thead>
 <tr>
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>No log</td>
      <td>0.459894</td>
      <td>0.856000</td>
    </tr>
    <tr>
      <td>2</td>
      <td>No log</td>
      <td>0.383288</td>
      <td>0.870000</td>
    </tr>
  </tbody>
</table><p>
</p></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.0001</code></pre>
</div>
<div class="cell-output cell-output-display">


    <div>
      
      <progress value="64" max="64" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [64/64 00:12, Epoch 2/2]
    </div>
    <table class="dataframe table table-sm table-striped">
  <thead>
 <tr>
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>No log</td>
      <td>0.106224</td>
      <td>0.964000</td>
    </tr>
    <tr>
      <td>2</td>
      <td>No log</td>
      <td>0.087845</td>
      <td>0.972000</td>
    </tr>
  </tbody>
</table><p>
</p></div>
</div>
<p>After many trials, I concluded that other models do not provide a performance benefit significant enough to make up for the time they consume to train. Also, the model seems to perform better when padding our data using <code>data_collator</code> than not.</p>
<p>The best learning rate for a batch size of 32 seems to be 1e-4, so I trained with the whole training dataset using these hyperparameters:</p>
<div class="cell" data-execution_count="86">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">trainer <span class="op" style="color: #5E5E5E;">=</span> get_trainer(model_name, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1e-4</span>, bs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">32</span>, epochs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, data_collator<span class="op" style="color: #5E5E5E;">=</span>data_collator)</span>
<span id="cb40-2">trainer.train()</span></code></pre></div>
<div class="cell-output cell-output-display">


    <div>
      
      <progress value="364" max="364" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [364/364 01:03, Epoch 2/2]
    </div>
    <table class="dataframe table table-sm table-striped">
  <thead>
 <tr>
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>No log</td>
      <td>0.070679</td>
      <td>0.978272</td>
    </tr>
    <tr>
      <td>2</td>
      <td>No log</td>
      <td>0.056363</td>
      <td>0.984480</td>
    </tr>
  </tbody>
</table><p>
</p></div>
<div class="cell-output cell-output-display" data-execution_count="86">
<pre><code>TrainOutput(global_step=364, training_loss=0.09329656454233023, metrics={'train_runtime': 63.2764, 'train_samples_per_second': 183.26, 'train_steps_per_second': 5.753, 'total_flos': 1309037659010832.0, 'train_loss': 0.09329656454233023, 'epoch': 2.0})</code></pre>
</div>
</div>
<div class="cell" data-execution_count="87">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">trainer.evaluate()</span></code></pre></div>
<div class="cell-output cell-output-display">

    <div>
      
      <progress value="61" max="61" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [61/61 00:03]
    </div>
    
</div>
<div class="cell-output cell-output-display" data-execution_count="87">
<pre><code>{'eval_loss': 0.05636342242360115,
 'eval_accuracy': 0.9844800827728919,
 'eval_runtime': 3.1079,
 'eval_samples_per_second': 621.973,
 'eval_steps_per_second': 19.628,
 'epoch': 2.0}</code></pre>
</div>
</div>
<p>Accuracy of 98.4%! We will save the model using <code>save_model</code> to use in our demo:</p>
<div class="cell" data-execution_count="88">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">trainer.save_model(<span class="st" style="color: #20794D;">"./model"</span>)</span></code></pre></div>
</div>
<p>Hugging face <a href="https://huggingface.co/docs/transformers/main_classes/pipelines">pipelines</a> simplify inference. The code block below uses the model that we trained above to determine whether “Today is a great day!” and “I have no motivation to do anything. I feel useless.” show signs of depression (or more accurately, how similar they are to posts written in r/depression).</p>
<div class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> pipeline</span>
<span id="cb45-2"></span>
<span id="cb45-3">examples <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"Today is a great day!"</span>, <span class="st" style="color: #20794D;">"I have no motivation to do anything. I feel useless."</span>]</span>
<span id="cb45-4">pipe <span class="op" style="color: #5E5E5E;">=</span> pipeline(<span class="st" style="color: #20794D;">"text-classification"</span>, model<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"./model"</span>, tokenizer<span class="op" style="color: #5E5E5E;">=</span>tokenizer)</span>
<span id="cb45-5">pipe(examples)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="92">
<pre><code>[{'label': 'LABEL_0', 'score': 0.9913545250892639},
 {'label': 'LABEL_1', 'score': 0.9631195068359375}]</code></pre>
</div>
</div>
<p>In our case, ‘LABEL_1’ means is_depression is 1 and ‘LABEL_0’ means otherwise. I will convert these values to true and false then convert the output of the pipe to a <code>{label: score}</code> dictionary, since that is what gradio requires.</p>
<div class="cell" data-execution_count="93">
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><span class="kw" style="color: #003B4F;">def</span> is_depression(txt):</span>
<span id="cb47-2">    pred_dict <span class="op" style="color: #5E5E5E;">=</span> pipe(txt)</span>
<span id="cb47-3">    <span class="cf" style="color: #003B4F;">for</span> d <span class="kw" style="color: #003B4F;">in</span> pred_dict: </span>
<span id="cb47-4">        d[<span class="st" style="color: #20794D;">'label'</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">False</span> <span class="cf" style="color: #003B4F;">if</span> d[<span class="st" style="color: #20794D;">'label'</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'LABEL_0'</span> <span class="cf" style="color: #003B4F;">else</span> <span class="va" style="color: #111111;">True</span></span>
<span id="cb47-5">    <span class="cf" style="color: #003B4F;">return</span> [{item[<span class="st" style="color: #20794D;">'label'</span>]: item[<span class="st" style="color: #20794D;">'score'</span>]} <span class="cf" style="color: #003B4F;">for</span> item <span class="kw" style="color: #003B4F;">in</span> pred_dict]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="96">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1">is_depression([<span class="st" style="color: #20794D;">"The tacos I ate today were horrible"</span>, <span class="st" style="color: #20794D;">"I am losing interest in things I had enjoyed. I hate life"</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="96">
<pre><code>[{False: 0.9979074001312256}, {True: 0.9663745164871216}]</code></pre>
</div>
</div>
<p>Gradio supports 1 input and 1 output (as far as I am aware), so our function shouldn’t return a list:</p>
<div class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><span class="kw" style="color: #003B4F;">def</span> predict_gradio(txt):</span>
<span id="cb50-2">    <span class="cf" style="color: #003B4F;">return</span> is_depression(txt)[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
</div>
<p>Finally, we can create our interface:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><span class="im" style="color: #00769E;">import</span> gradio <span class="im" style="color: #00769E;">as</span> gr</span>
<span id="cb51-2"></span>
<span id="cb51-3">title <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"Depression Classifier"</span></span>
<span id="cb51-4">description <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"A NLP classifier trained with Hugging Face Transformers."</span></span>
<span id="cb51-5">interpretation<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'default'</span></span>
<span id="cb51-6">enable_queue<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span></span>
<span id="cb51-7"></span>
<span id="cb51-8">gr.Interface(fn<span class="op" style="color: #5E5E5E;">=</span>predict_gradio, inputs<span class="op" style="color: #5E5E5E;">=</span>gr.inputs.Textbox(label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Text"</span>), outputs<span class="op" style="color: #5E5E5E;">=</span>gr.outputs.Label(label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"is_depression"</span>), title<span class="op" style="color: #5E5E5E;">=</span>title,description<span class="op" style="color: #5E5E5E;">=</span>description,article<span class="op" style="color: #5E5E5E;">=</span>article,examples<span class="op" style="color: #5E5E5E;">=</span>examples,interpretation<span class="op" style="color: #5E5E5E;">=</span>interpretation,enable_queue<span class="op" style="color: #5E5E5E;">=</span>enable_queue).launch(share<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<p>I built an web application hosted <a href="https://se0ngbin.github.io/depression-classifier/">here</a> using this gradio api! On how to do this, refer to <a href="https://tmabraham.github.io/blog/gradio_hf_spaces_tutorial">this</a> article.</p>


</section>

 ]]></description>
  <category>ml</category>
  <category>projects</category>
  <guid>https://blog.seongbin.me/posts/depression-predictor/index.html</guid>
  <pubDate>Thu, 01 Sep 2022 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Cross Entropy Loss Explained</title>
  <dc:creator>Seongbin Park</dc:creator>
  <link>https://blog.seongbin.me/posts/cross-entropy-loss/index.html</link>
  <description><![CDATA[ 




<p>Cross entropy loss is a loss function that can be used for multi-class classification using neural networks. <a href="https://github.com/fastai/fastbook/blob/master/05_pet_breeds.ipynb">Chapter 5</a> of the fast.ai textbook outlines the use of cross entropy loss for binary classification, so in this post, we will take a look at classification for 3 classes.</p>
<div class="cell">
<details>
<summary>required libraries</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> fastai.vision.<span class="bu" style="color: null;">all</span> <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span>
<span id="cb1-2">torch.random.manual_seed(<span class="dv" style="color: #AD0000;">42</span>)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
</details>
</div>
<section id="softmax" class="level2">
<h2 class="anchored" data-anchor-id="softmax">Softmax</h2>
<p>The softmax function ensures 2 things: - activations are all between 0 and 1 - activations sum to 1.</p>
<p>For multi-class classification, we need an activation per class (in the final layer). Each activation then indicates the relative confidence of each class being the true label. Therefore, we can get the predicted prababilities that each class is the true label by applying the softmax function to the final column of activations.</p>
<p>Given <img src="https://latex.codecogs.com/png.latex?C"> total classes, for any class <img src="https://latex.codecogs.com/png.latex?k,"> let’s say <img src="https://latex.codecogs.com/png.latex?x_k"> represents the activation for <img src="https://latex.codecogs.com/png.latex?c">. Then, the softmax activation for an arbitrary class <img src="https://latex.codecogs.com/png.latex?c"> is equal to</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Be%5E%7Bx_c%7D%7D%7B%5Csum%5EC_%7Bk=1%7De%5E%7Bx_k%7D%7D."></p>
<p>In Python code, this would be</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;">def</span> softmax(x): <span class="cf" style="color: #003B4F;">return</span> exp(x) <span class="op" style="color: #5E5E5E;">/</span> exp(x).<span class="bu" style="color: null;">sum</span>(dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, keepdim<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
<p>Note that the code version returns a tensor/array of softmax activations.</p>
<p>For demonstration purposes, let’s first create a set of activations using <code>torch.randn</code>, assuming we have 6 objects to classify into 3 classes.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">acts <span class="op" style="color: #5E5E5E;">=</span> torch.randn((<span class="dv" style="color: #AD0000;">6</span>,<span class="dv" style="color: #AD0000;">3</span>))<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb3-2">acts</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>tensor([[ 3.8538,  2.9746, -0.9948],
        [ 0.8792, -1.5163,  2.1566],
        [ 1.6016,  3.3612,  0.7117],
        [-1.3732,  1.2209,  2.6695],
        [-0.4632,  0.0835, -0.5032],
        [ 1.7197, -0.6195, -0.7914]])</code></pre>
</div>
</div>
<p>Let’s also set our target labels:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">targ <span class="op" style="color: #5E5E5E;">=</span> tensor([<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">0</span>])</span></code></pre></div>
</div>
<p>To take the softmax of our initial (random) activations, we need to pass <code>acts</code> into <code>torch.softmax</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">sm_acts <span class="op" style="color: #5E5E5E;">=</span> torch.softmax(acts, dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb6-2">sm_acts</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>tensor([[0.7028, 0.2917, 0.0055],
        [0.2137, 0.0195, 0.7668],
        [0.1385, 0.8046, 0.0569],
        [0.0140, 0.1876, 0.7984],
        [0.2711, 0.4684, 0.2605],
        [0.8492, 0.0819, 0.0689]])</code></pre>
</div>
</div>
<p>Perfect! We can check that each row adds up to 1 as expected.</p>
</section>
<section id="log-likelihood" class="level2">
<h2 class="anchored" data-anchor-id="log-likelihood">Log Likelihood</h2>
<p>To calculate our loss, for each item of <code>targ</code>, we need to select the appropriate column of <code>sm_acts</code> using tensor indexing:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">idx <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">6</span>)</span>
<span id="cb8-2">sm_acts[idx, targ]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>tensor([0.7028, 0.0195, 0.1385, 0.7984, 0.2605, 0.8492])</code></pre>
</div>
</div>
<p><code>F.nll_loss</code> does the same thing, but flips the sign of each number in the tensor. PyTorch defaults to taking the mean of the losses; to prevent this, we can pass <code>reduction='none'</code> as a parameter.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">result <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span>F.nll_loss(sm_acts, targ, reduction<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'none'</span>)</span>
<span id="cb10-2">result</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>tensor([0.7028, 0.0195, 0.1385, 0.7984, 0.2605, 0.8492])</code></pre>
</div>
</div>
<section id="taking-the-logarithm" class="level3">
<h3 class="anchored" data-anchor-id="taking-the-logarithm">Taking the Logarithm</h3>
<p>We take the (natural) logarithm of <code>result</code> for two reasons: - prevents under/overflow when performing mathematical operations - differences between small numbers is amplified</p>
<p>In our case, <code>result</code> relfects the predicted probability of the correct label, so when the prediction is “good” (closer to 1), we want our loss function to return a small value (and vice versa). We can achieve this by taking the negative of the log:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">loss <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span>torch.log(result)</span>
<span id="cb12-2">loss</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>tensor([0.3527, 3.9384, 1.9770, 0.2251, 1.3451, 0.1635])</code></pre>
</div>
</div>
<p>And there we go! We just found the cross entropy loss for our example.</p>
</section>
</section>
<section id="using-modules" class="level1">
<h1>Using Modules</h1>
<p>We can simplify the code above by using <code>log_softmax</code> followed <code>nll_loss</code>:</p>
<div class="cell" data-scrolled="true">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">lsm_acts <span class="op" style="color: #5E5E5E;">=</span> F.log_softmax(acts, dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb14-2">loss <span class="op" style="color: #5E5E5E;">=</span> F.nll_loss(lsm_acts, targ, reduction<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'none'</span>)</span>
<span id="cb14-3">loss</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>tensor([0.3527, 3.9384, 1.9770, 0.2251, 1.3451, 0.1635])</code></pre>
</div>
</div>
<p>In practice, this is exactly what <code>nn.CrossEntropyLoss</code> does:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">nn.CrossEntropyLoss(reduction<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'none'</span>)(acts, targ)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>tensor([0.3527, 3.9384, 1.9770, 0.2251, 1.3451, 0.1635])</code></pre>
</div>
</div>
<p>The output loss tensors for all three approaches are equivalent as expected!</p>


</section>

 ]]></description>
  <category>ml</category>
  <category>notes</category>
  <guid>https://blog.seongbin.me/posts/cross-entropy-loss/index.html</guid>
  <pubDate>Sat, 06 Aug 2022 07:00:00 GMT</pubDate>
</item>
<item>
  <title>MNIST Digit Classifier</title>
  <dc:creator>Seongbin Park</dc:creator>
  <link>https://blog.seongbin.me/posts/digit-classifier/index.html</link>
  <description><![CDATA[ 




<p>This post will cover how to classify handwritten digits of the MNIST dataset using a simple neural network. At the same time, I will be taking a stab at the <a href="https://www.kaggle.com/competitions/digit-recognizer/overview">Kaggle Digit Recognizer</a> contest.</p>
<p>Credits: I will be working off of chapter 4 of the <a href="https://github.com/fastai/fastbook">fast.ai</a> book, which covers binary classification of 3’s and 7’s. Other resources are linked.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>required libraries</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> fastai.vision.<span class="bu" style="color: null;">all</span> <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span></code></pre></div>
</details>
</div>
<section id="dowloading-the-data" class="level2">
<h2 class="anchored" data-anchor-id="dowloading-the-data">Dowloading the Data</h2>
<p>First, we will have to import the MNIST dataset itself. We can import it using the fast.ai library (<code>path = untar_data(URLs.MNIST)</code>), but I will download the dataset from <a href="https://www.kaggle.com/competitions/digit-recognizer/">kaggle</a> instead.</p>
<p>If you are following along and haven’t set up the kaggle API yet, do so by following along the README of the official <a href="https://github.com/Kaggle/kaggle-api">repo</a>. You will need an account to do so. After everything is set up, we can run the following code block:</p>
<div class="cell" data-scrolled="true" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="op" style="color: #5E5E5E;">!</span>kaggle competitions download <span class="op" style="color: #5E5E5E;">-</span>c digit<span class="op" style="color: #5E5E5E;">-</span>recognizer</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading digit-recognizer.zip to /home/jupyter/projects/digit-classifier
  0%|                                               | 0.00/15.3M [00:00&lt;?, ?B/s]
100%|███████████████████████████████████████| 15.3M/15.3M [00:00&lt;00:00, 161MB/s]</code></pre>
</div>
</div>
<p>Note that in Jupyter notebooks, the exclamation mark ! is used to execute shell commands. The dataset should be downloaded in your project directory as a zip file. Run the following code block to extract the contents to a file named MNIST_dataset:</p>
<div class="cell" data-scrolled="true" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="op" style="color: #5E5E5E;">!</span>unzip digit<span class="op" style="color: #5E5E5E;">-</span>recognizer.<span class="bu" style="color: null;">zip</span> <span class="op" style="color: #5E5E5E;">-</span>d MNIST_dataset</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Archive:  digit-recognizer.zip
  inflating: MNIST_dataset/sample_submission.csv  
  inflating: MNIST_dataset/test.csv  
  inflating: MNIST_dataset/train.csv  </code></pre>
</div>
</div>
<p>Let’s take a look at <code>test.csv</code> (the test set) and <code>train.csv</code> (the training set):</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">ds_path <span class="op" style="color: #5E5E5E;">=</span> Path(<span class="st" style="color: #20794D;">"./MNIST_dataset"</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;"># test.csv</span></span>
<span id="cb7-2">df_test <span class="op" style="color: #5E5E5E;">=</span> pd.read_csv(ds_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">"test.csv"</span>)</span>
<span id="cb7-3">df_test.head(<span class="dv" style="color: #AD0000;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>pixel0</th>
      <th>pixel1</th>
      <th>pixel2</th>
      <th>pixel3</th>
      <th>pixel4</th>
      <th>pixel5</th>
      <th>pixel6</th>
      <th>pixel7</th>
      <th>pixel8</th>
      <th>pixel9</th>
      <th>...</th>
      <th>pixel774</th>
      <th>pixel775</th>
      <th>pixel776</th>
      <th>pixel777</th>
      <th>pixel778</th>
      <th>pixel779</th>
      <th>pixel780</th>
      <th>pixel781</th>
      <th>pixel782</th>
      <th>pixel783</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 784 columns</p>
</div>
</div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="co" style="color: #5E5E5E;"># train.csv</span></span>
<span id="cb8-2">df_train <span class="op" style="color: #5E5E5E;">=</span> pd.read_csv(ds_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">"train.csv"</span>)</span>
<span id="cb8-3">df_train.head(<span class="dv" style="color: #AD0000;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>label</th>
      <th>pixel0</th>
      <th>pixel1</th>
      <th>pixel2</th>
      <th>pixel3</th>
      <th>pixel4</th>
      <th>pixel5</th>
      <th>pixel6</th>
      <th>pixel7</th>
      <th>pixel8</th>
      <th>...</th>
      <th>pixel774</th>
      <th>pixel775</th>
      <th>pixel776</th>
      <th>pixel777</th>
      <th>pixel778</th>
      <th>pixel779</th>
      <th>pixel780</th>
      <th>pixel781</th>
      <th>pixel782</th>
      <th>pixel783</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 785 columns</p>
</div>
</div>
</div>
<p>Now that we downloaded the data, we need to shape it for training and validating.</p>
</section>
<section id="shaping-the-data" class="level2">
<h2 class="anchored" data-anchor-id="shaping-the-data">Shaping the Data</h2>
<p>To train our model, we need to separate and normalize the independent (pixels) and dependent (label) variables. The labels will be represented using <a href="https://en.wikipedia.org/wiki/One-hot">one hot encoding</a>.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">X_train <span class="op" style="color: #5E5E5E;">=</span> tensor(df_train.drop(labels <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">'label'</span>],axis <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span>)) <span class="op" style="color: #5E5E5E;">/</span> <span class="fl" style="color: #AD0000;">255.0</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">y_train_numeric <span class="op" style="color: #5E5E5E;">=</span> df_train[<span class="st" style="color: #20794D;">'label'</span>]</span>
<span id="cb10-2"></span>
<span id="cb10-3">rows <span class="op" style="color: #5E5E5E;">=</span> np.arange(y_train_numeric.size)</span>
<span id="cb10-4">y_train <span class="op" style="color: #5E5E5E;">=</span> tensor(np.zeros((y_train_numeric.size, <span class="dv" style="color: #AD0000;">10</span>)))</span>
<span id="cb10-5">y_train[rows, y_train_numeric] <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb10-6"></span>
<span id="cb10-7">X_train.shape, y_train.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>(torch.Size([42000, 784]), torch.Size([42000, 10]))</code></pre>
</div>
</div>
<p><code>X_train.shape</code> and <code>y_train.shape</code> tells us that we have 42000 digits in our dataset, with each digit having 784 pixels. We will use tensors to take advantage of faster GPU computations.</p>
<p>We want to create a Pytorch <a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"><code>Dataset</code></a>, which is required to return a tuple of <code>(x,y)</code> when indexed. Python provides a <a href="https://www.w3schools.com/python/ref_func_zip.asp"><code>zip</code></a> function which, when combined with <a href="https://www.w3schools.com/python/ref_func_list.asp"><code>list</code></a>, can do this easily:</p>
<div class="cell" data-scrolled="true" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">ds <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(<span class="bu" style="color: null;">zip</span>(X_train,y_train))</span>
<span id="cb12-2">ds[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb12-3"><span class="co" style="color: #5E5E5E;"># output removed for readability</span></span></code></pre></div>
</div>
<p>Next, we want to split our dataset <code>ds</code> into a training and validation set:</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">train, val <span class="op" style="color: #5E5E5E;">=</span> torch.utils.data.random_split(ds,[<span class="dv" style="color: #AD0000;">32000</span>, <span class="dv" style="color: #AD0000;">10000</span>])</span></code></pre></div>
</div>
<p>Later, we will be using stochastic gradient descent, which requires that we have “mini-batches” of our dataset. We can create a <code>DataLoader</code> from our <code>train</code> dataset to do so:</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(train, batch_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">256</span>)</span>
<span id="cb14-2">xb,yb <span class="op" style="color: #5E5E5E;">=</span> first(dl)</span>
<span id="cb14-3">xb.shape,yb.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>(torch.Size([256, 784]), torch.Size([256, 10]))</code></pre>
</div>
</div>
<p>We can do the same for our validation (<code>val</code>) dataset:</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">valid_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(val, batch_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">256</span>)</span></code></pre></div>
</div>
</section>
<section id="training-a-linear-model" class="level2">
<h2 class="anchored" data-anchor-id="training-a-linear-model">Training a Linear Model</h2>
<p>Now that our data is ready, we can start training our classification model. We will start with a linear model, then add some non-linearity to it!</p>
<p>First, we must randomly initialize the bias and all weights for each pixel. Since we have 10 labels (one for each digit), there must be 10 outputs, so our weights matrix is of size <code>784x10</code>.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="kw" style="color: #003B4F;">def</span> init_params(size, std<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1.0</span>): <span class="cf" style="color: #003B4F;">return</span> (torch.randn(size)<span class="op" style="color: #5E5E5E;">*</span>std).requires_grad_()</span>
<span id="cb17-2"></span>
<span id="cb17-3">weights <span class="op" style="color: #5E5E5E;">=</span> init_params((<span class="dv" style="color: #AD0000;">784</span>,<span class="dv" style="color: #AD0000;">10</span>))</span>
<span id="cb17-4">bias <span class="op" style="color: #5E5E5E;">=</span> init_params(<span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</div>
<p>The prediction given a tensor <code>x</code> is</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bprediction%7D%20=%20x%20%5Ccdot%20%5Ctext%7Bweights%7D%20+%20%5Ctext%7Bbias%7D."></p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="kw" style="color: #003B4F;">def</span> linear1(xb): <span class="cf" style="color: #003B4F;">return</span> xb<span class="op" style="color: #5E5E5E;">@</span>weights <span class="op" style="color: #5E5E5E;">+</span> bias</span></code></pre></div>
</div>
<p>To calculate a gradient, we need a loss function. Since there are more than 2 labels, we will use <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">cross entropy loss</a>, which is related to the <a href="https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax">softmax</a> function instead of a sigmoid function (which is used for binary classification).</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="kw" style="color: #003B4F;">def</span> mnist_loss(xb, yb):</span>
<span id="cb19-2">    loss <span class="op" style="color: #5E5E5E;">=</span> nn.CrossEntropyLoss()</span>
<span id="cb19-3">    <span class="cf" style="color: #003B4F;">return</span> loss(xb, yb)</span></code></pre></div>
</div>
<p>For testing and demonstration purposes, let’s work with a smaller batch than the ones created when shaping our data.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">batch <span class="op" style="color: #5E5E5E;">=</span> X_train[:<span class="dv" style="color: #AD0000;">4</span>]</span>
<span id="cb20-2">batch.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>torch.Size([4, 784])</code></pre>
</div>
</div>
<div class="cell" data-scrolled="true" data-execution_count="17">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">preds <span class="op" style="color: #5E5E5E;">=</span> linear1(batch)</span>
<span id="cb22-2">preds</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>tensor([[ -0.5287,  12.6047,  -9.0290,  -1.7505,   3.7686,  18.8489,  -1.8141,
         -12.2232,  -5.1421,  -6.6316],
        [ 18.8942,  10.7898,   9.2573,   7.9989,  -1.2884,  19.0238,  -5.8788,
           6.5045, -10.2431,   5.5865],
        [  6.3639,  14.0687,   0.7705,  -1.3580,   1.4220,   7.3108,  -7.4359,
          -6.8101,  -5.9212,  23.7016],
        [-14.7847,   3.0711,  -0.6092,   2.2720,  -1.1361,   3.7617,   5.1197,
           5.3868,  -1.5228,  -7.6523]], grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
<div class="cell" data-scrolled="true" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">loss <span class="op" style="color: #5E5E5E;">=</span> mnist_loss(preds, y_train[:<span class="dv" style="color: #AD0000;">4</span>])</span>
<span id="cb24-2">loss</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>tensor(5.9773, grad_fn=&lt;DivBackward1&gt;)</code></pre>
</div>
</div>
<p>Now we can calculate the gradients:</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">loss.backward()</span>
<span id="cb26-2">weights.grad.shape,weights.grad.mean(),bias.grad</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>(torch.Size([784, 10]), tensor(2.4328e-10), tensor([5.9605e-08]))</code></pre>
</div>
</div>
<p>The following function combines the above code and generalizes to models other than <code>linear1</code>.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><span class="kw" style="color: #003B4F;">def</span> calc_grad(xb, yb, model):</span>
<span id="cb28-2">    preds <span class="op" style="color: #5E5E5E;">=</span> model(xb)</span>
<span id="cb28-3">    loss <span class="op" style="color: #5E5E5E;">=</span> mnist_loss(preds, yb)</span>
<span id="cb28-4">    loss.backward()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">calc_grad(batch, y_train[:<span class="dv" style="color: #AD0000;">4</span>], linear1)</span>
<span id="cb29-2">weights.grad.mean(),bias.grad</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>(tensor(4.8657e-10), tensor([1.1921e-07]))</code></pre>
</div>
</div>
<p>Using the calculated gradients, we can update the weights for each epoch. We need to specify a learning rate and reset the gradients to 0, since <code>loss.backward</code> actually adds the gradients of loss to any gradients that are currently stored.</p>
<div class="sourceCode" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">lr <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1.</span></span>
<span id="cb31-2"></span>
<span id="cb31-3"><span class="cf" style="color: #003B4F;">for</span> p <span class="kw" style="color: #003B4F;">in</span> weights,bias:</span>
<span id="cb31-4">    p.data <span class="op" style="color: #5E5E5E;">-=</span> p.grad<span class="op" style="color: #5E5E5E;">*</span>lr</span>
<span id="cb31-5">    p.grad.zero_()</span></code></pre></div>
<p>Finally, we can define a function that trains the model for one epoch:</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><span class="kw" style="color: #003B4F;">def</span> train_epoch(model, params, lr<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>):</span>
<span id="cb32-2">    <span class="cf" style="color: #003B4F;">for</span> xb,yb <span class="kw" style="color: #003B4F;">in</span> dl:</span>
<span id="cb32-3">        calc_grad(xb, yb, model)</span>
<span id="cb32-4">        <span class="cf" style="color: #003B4F;">for</span> p <span class="kw" style="color: #003B4F;">in</span> params:</span>
<span id="cb32-5">            p.data <span class="op" style="color: #5E5E5E;">-=</span> p.grad<span class="op" style="color: #5E5E5E;">*</span>lr</span>
<span id="cb32-6">            p.grad.zero_()</span></code></pre></div>
</div>
<p>We also probably want to check the accuracy of our model. The label that the model predicts is the label with the highest activation:</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><span class="kw" style="color: #003B4F;">def</span> batch_accuracy(xb, yb):</span>
<span id="cb33-2">    label <span class="op" style="color: #5E5E5E;">=</span> torch.argmax(xb, dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb33-3">    y_truth <span class="op" style="color: #5E5E5E;">=</span> torch.argmax(yb, dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb33-4">    correct <span class="op" style="color: #5E5E5E;">=</span> y_truth <span class="op" style="color: #5E5E5E;">==</span> label</span>
<span id="cb33-5">    <span class="cf" style="color: #003B4F;">return</span> correct.<span class="bu" style="color: null;">float</span>().mean()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">batch_accuracy(linear1(batch), y_train[:<span class="dv" style="color: #AD0000;">4</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>tensor(0.)</code></pre>
</div>
</div>
<p>To get the accuracy for the whole epoch, we must call <code>batch_accuracy</code> with batches of the validation dataset, then take the mean over all batches.</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><span class="kw" style="color: #003B4F;">def</span> validate_epoch(model):</span>
<span id="cb36-2">    accs <span class="op" style="color: #5E5E5E;">=</span> [batch_accuracy(model(xb), yb) <span class="cf" style="color: #003B4F;">for</span> xb,yb <span class="kw" style="color: #003B4F;">in</span> valid_dl]</span>
<span id="cb36-3">    <span class="cf" style="color: #003B4F;">return</span> <span class="bu" style="color: null;">round</span>(torch.stack(accs).mean().item(), <span class="dv" style="color: #AD0000;">4</span>)</span></code></pre></div>
</div>
<p>Finally, we can see if our code works by checking if the accuracy improves!</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1">params <span class="op" style="color: #5E5E5E;">=</span> weights, bias</span>
<span id="cb37-2"><span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">40</span>):</span>
<span id="cb37-3">    train_epoch(linear1, params)</span>
<span id="cb37-4">    <span class="bu" style="color: null;">print</span>(validate_epoch(linear1), end<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">' '</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.8213 0.8532 0.8661 0.8736 0.8769 0.8808 0.8848 0.8869 0.8895 0.8914 0.8929 0.894 0.8949 0.8964 0.8973 0.8977 0.8978 0.8979 0.8981 0.8986 0.8998 0.9 0.9004 0.9016 0.9017 0.9027 0.9027 0.9032 0.9037 0.9047 0.9053 0.9058 0.9061 0.9062 0.9061 0.9062 0.906 0.9061 0.906 0.9063 </code></pre>
</div>
</div>
</section>
<section id="simplifying-code" class="level2">
<h2 class="anchored" data-anchor-id="simplifying-code">Simplifying Code</h2>
<p><code>nn.Linear</code> does the same thing as our <code>init_params</code> and <code>linear1</code> functions together. Also, fastai’s <code>SGD</code> class provides us with functions that takes care of updating the parameters and reseting the gradients of our model. By replacing some code, we can boil the training portion of our MNIST classifer down to the following:</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><span class="kw" style="color: #003B4F;">def</span> mnist_loss(xb, yb):</span>
<span id="cb39-2">    loss <span class="op" style="color: #5E5E5E;">=</span> nn.CrossEntropyLoss()</span>
<span id="cb39-3">    <span class="cf" style="color: #003B4F;">return</span> loss(xb, yb)</span>
<span id="cb39-4"></span>
<span id="cb39-5"><span class="kw" style="color: #003B4F;">def</span> calc_grad(xb, yb, model):</span>
<span id="cb39-6">    preds <span class="op" style="color: #5E5E5E;">=</span> model(xb)</span>
<span id="cb39-7">    loss <span class="op" style="color: #5E5E5E;">=</span> mnist_loss(preds, yb)</span>
<span id="cb39-8">    loss.backward()</span>
<span id="cb39-9"></span>
<span id="cb39-10"><span class="kw" style="color: #003B4F;">def</span> train_epoch_simple(model):</span>
<span id="cb39-11">    <span class="cf" style="color: #003B4F;">for</span> xb,yb <span class="kw" style="color: #003B4F;">in</span> dl:</span>
<span id="cb39-12">        calc_grad(xb, yb, model)</span>
<span id="cb39-13">        opt.step()</span>
<span id="cb39-14">        opt.zero_grad()</span>
<span id="cb39-15">        </span>
<span id="cb39-16"><span class="kw" style="color: #003B4F;">def</span> train_model(model, epochs):</span>
<span id="cb39-17">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb39-18">        train_epoch_simple(model)</span>
<span id="cb39-19">        <span class="bu" style="color: null;">print</span>(validate_epoch(model), end<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">' '</span>)</span>
<span id="cb39-20">        </span>
<span id="cb39-21">linear_model <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(<span class="dv" style="color: #AD0000;">28</span><span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">28</span>,<span class="dv" style="color: #AD0000;">10</span>)</span>
<span id="cb39-22">opt <span class="op" style="color: #5E5E5E;">=</span> SGD(linear_model.parameters(), lr<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb39-23">train_model(linear_model, <span class="dv" style="color: #AD0000;">20</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.8983 0.9057 0.9092 0.9113 0.9143 0.9149 0.9154 0.9154 0.9162 0.9161 0.9166 0.9167 0.9166 0.9166 0.9164 0.9169 0.917 0.9173 0.9173 0.9175 </code></pre>
</div>
</div>
<p>Fast.ai provides us with <code>Learner.fit</code>, which we can use instead of <code>train_model</code> to significantly reduce the amount of code we need to write. To use the function, we must create a <code>Learner</code>, which requires a <code>DataLoaders</code> of our training and validation datasets:</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">dls <span class="op" style="color: #5E5E5E;">=</span> DataLoaders(dl, valid_dl)</span></code></pre></div>
</div>
<p>Then, we pass in <code>DataLoaders</code>, the model, the optimization function, the loss function, and optionally any metrics to print into the <code>Learner</code> constructor to create one:</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">learn <span class="op" style="color: #5E5E5E;">=</span> Learner(dls, nn.Linear(<span class="dv" style="color: #AD0000;">28</span><span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">28</span>,<span class="dv" style="color: #AD0000;">10</span>), opt_func<span class="op" style="color: #5E5E5E;">=</span>SGD,</span>
<span id="cb42-2">                loss_func<span class="op" style="color: #5E5E5E;">=</span>mnist_loss, metrics<span class="op" style="color: #5E5E5E;">=</span>batch_accuracy)</span></code></pre></div>
</div>
<p>Finally, we can call <code>Learner.fit</code>:</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">learn.fit(<span class="dv" style="color: #AD0000;">10</span>, lr<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>batch_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.413918</td>
      <td>0.365917</td>
      <td>0.897400</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.326721</td>
      <td>0.337396</td>
      <td>0.905100</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.302027</td>
      <td>0.325679</td>
      <td>0.908500</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.289524</td>
      <td>0.319092</td>
      <td>0.910000</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.281248</td>
      <td>0.314855</td>
      <td>0.912800</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.275084</td>
      <td>0.311911</td>
      <td>0.913200</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.270192</td>
      <td>0.309764</td>
      <td>0.913500</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.266153</td>
      <td>0.308146</td>
      <td>0.913900</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.262723</td>
      <td>0.306901</td>
      <td>0.914300</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.259748</td>
      <td>0.305928</td>
      <td>0.914900</td>
      <td>00:00</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</section>
<section id="adding-non-linearity" class="level2">
<h2 class="anchored" data-anchor-id="adding-non-linearity">Adding Non-linearity</h2>
<p>To expand upon our model, we can add another layer on top of what we have now. However, mathematically speaking, the composition of two linear functions is another linear function. Therefore, stacking two linear classifiers on top of each other is equivalent to having just one linear classifier.</p>
<p>Therefore, we must add some non-linearity between linear layers. We often do this by through activation functions; a common one is the <a href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/"><code>ReLU</code></a> function:</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">x <span class="op" style="color: #5E5E5E;">=</span> np.linspace(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">5</span>,<span class="dv" style="color: #AD0000;">5</span>,<span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb44-2">y <span class="op" style="color: #5E5E5E;">=</span> np.maximum(<span class="dv" style="color: #AD0000;">0</span>, x)</span>
<span id="cb44-3"></span>
<span id="cb44-4">plt.plot(x, y)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://blog.seongbin.me/posts/digit-classifier/index_files/figure-html/cell-32-output-1.png" class="img-fluid"></p>
</div>
</div>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html"><code>nn.Sequential</code></a> creates a module that will call each of the listed layers or functions.</p>
<p>Our first layer takes in 784 inputs (pixels) and outputs 60 numbers. Those 60 numbers are then each passed into the <code>ReLU</code> function before going into the second layer. The second layer has 10 outputs, which as before, is the probability of each digit being the lable.</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1">simple_net <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(</span>
<span id="cb45-2">    nn.Linear(<span class="dv" style="color: #AD0000;">784</span>,<span class="dv" style="color: #AD0000;">100</span>),</span>
<span id="cb45-3">    nn.ReLU(),</span>
<span id="cb45-4">    nn.Linear(<span class="dv" style="color: #AD0000;">100</span>,<span class="dv" style="color: #AD0000;">10</span>)</span>
<span id="cb45-5">)</span></code></pre></div>
</div>
<p>We can train this model using <code>Learner.fit</code> as well (we are using more epochs and smaller learning rate, since it is a larger model):</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1">learn <span class="op" style="color: #5E5E5E;">=</span> Learner(dls, simple_net, opt_func<span class="op" style="color: #5E5E5E;">=</span>SGD,</span>
<span id="cb46-2">                loss_func<span class="op" style="color: #5E5E5E;">=</span>mnist_loss, metrics<span class="op" style="color: #5E5E5E;">=</span>batch_accuracy)</span>
<span id="cb46-3">learn.fit(<span class="dv" style="color: #AD0000;">60</span>, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.1</span>)</span></code></pre></div>
</div>
<p>The output is ommitted to save room; the training process is recorded in <code>learn.recorder</code>, with the table of output stored in the <code>values</code> attribute, so we can plot the accuracy over training as:</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1">plt.plot(L(learn.recorder.values).itemgot(<span class="dv" style="color: #AD0000;">2</span>))</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://blog.seongbin.me/posts/digit-classifier/index_files/figure-html/cell-35-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><span class="co" style="color: #5E5E5E;"># final accuracy</span></span>
<span id="cb48-2">learn.recorder.values[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>][<span class="dv" style="color: #AD0000;">2</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>0.9652000069618225</code></pre>
</div>
</div>
</section>
<section id="making-a-submission" class="level2">
<h2 class="anchored" data-anchor-id="making-a-submission">Making a Submission</h2>
<p>Though our very basic model is far from perfect, we can still submit it to the competition! Recall that we stored the test.csv data into the df_test <code>DataFrame</code>. We need to first normalize the pixels then plug it into our model:</p>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1">X_test <span class="op" style="color: #5E5E5E;">=</span> tensor(df_test)<span class="op" style="color: #5E5E5E;">/</span> <span class="fl" style="color: #AD0000;">255.0</span></span></code></pre></div>
</div>
<div class="cell" data-scrolled="true" data-execution_count="37">
<div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1">x_val <span class="op" style="color: #5E5E5E;">=</span> simple_net(X_test)</span>
<span id="cb51-2">y_pred <span class="op" style="color: #5E5E5E;">=</span> torch.argmax(x_val, dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb51-3">y_pred <span class="op" style="color: #5E5E5E;">=</span> y_pred.tolist()</span></code></pre></div>
</div>
<p>Finally, we can create a submission file in our current directory:</p>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><span class="cf" style="color: #003B4F;">with</span> <span class="bu" style="color: null;">open</span>(<span class="st" style="color: #20794D;">"submission.csv"</span>, <span class="st" style="color: #20794D;">'w+'</span>) <span class="im" style="color: #00769E;">as</span> f :</span>
<span id="cb52-2">    f.write(<span class="st" style="color: #20794D;">'ImageId,Label</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">'</span>)</span>
<span id="cb52-3">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(y_pred)) :</span>
<span id="cb52-4">        f.write(<span class="st" style="color: #20794D;">""</span>.join([<span class="bu" style="color: null;">str</span>(i<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>),<span class="st" style="color: #20794D;">','</span>,<span class="bu" style="color: null;">str</span>(y_pred[i]),<span class="st" style="color: #20794D;">'</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">'</span>]))</span></code></pre></div>
</div>
<p>Then submit to Kaggle!</p>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb53" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><span class="op" style="color: #5E5E5E;">!</span>kaggle competitions submit <span class="op" style="color: #5E5E5E;">-</span>c digit<span class="op" style="color: #5E5E5E;">-</span>recognizer <span class="op" style="color: #5E5E5E;">-</span>f submission.csv <span class="op" style="color: #5E5E5E;">-</span>m <span class="st" style="color: #20794D;">"First Attempt"</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>100%|█████████████████████████████████████████| 208k/208k [00:01&lt;00:00, 165kB/s]
Successfully submitted to Digit Recognizer</code></pre>
</div>
</div>


</section>

 ]]></description>
  <category>ml</category>
  <category>projects</category>
  <category>kaggle-competition</category>
  <guid>https://blog.seongbin.me/posts/digit-classifier/index.html</guid>
  <pubDate>Fri, 05 Aug 2022 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Numpy Arrays vs. Pytorch Tensors</title>
  <dc:creator>Seongbin Park</dc:creator>
  <link>https://blog.seongbin.me/posts/arrays-vs-tensors/index.html</link>
  <description><![CDATA[ 




<p>Both NumPy arrays and PyTorch tensors can be viewed as multidimensional tables of data. By using them, we can speed up computations by many thousands of times compared to using pure Python. This post will clarify the difference between the two.</p>
<p><img src="https://blog.seongbin.me/posts/arrays-vs-tensors/arr-vs-ten.png" title="Credit: https://python.plainenglish.io/numpy-arrays-vs-tensorflow-tensors-95a9c39e1c17" class="img-fluid"></p>
<p>I will be using the fastai library for this post; the code will be partially based on chapter 4 of the <a href="https://github.com/fastai/fastbook">fastai book</a>.</p>
<div class="cell" data-outputid="4c7ac52c-32d4-48ea-9604-e57de48a9016" data-execution_count="3">
<details>
<summary>required libraries</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;">!</span> [ <span class="op" style="color: #5E5E5E;">-</span>e <span class="op" style="color: #5E5E5E;">/</span>content ] <span class="op" style="color: #5E5E5E;">&amp;&amp;</span> pip install <span class="op" style="color: #5E5E5E;">-</span>Uqq fastbook</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> fastbook</span>
<span id="cb1-3">fastbook.setup_book()</span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="im" style="color: #00769E;">from</span> fastai.vision.<span class="bu" style="color: null;">all</span> <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span>
<span id="cb1-6"><span class="im" style="color: #00769E;">from</span> fastbook <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span>
<span id="cb1-7"></span>
<span id="cb1-8">matplotlib.rc(<span class="st" style="color: #20794D;">'image'</span>, cmap<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'Greys'</span>)</span></code></pre></div>
</details>
</div>
<section id="basics" class="level1">
<h1>Basics</h1>
<section id="numpy-arrays" class="level2">
<h2 class="anchored" data-anchor-id="numpy-arrays">NumPy Arrays</h2>
<p>All items in a NumPy array must be of the same type. Arrays are mutable, which means that we can change the values of each item in the array.</p>
<p>The innermost arrays of multidimensional arrays can have varying sizes—this is called a “jagged array.”</p>
<p>Most functions supported by NumPy arrays are supported by PyTorch tensors.</p>
</section>
<section id="pytorch-tensor" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-tensor">Pytorch Tensor</h2>
<p>All items in a PyTorch tensor must also be of the same type, but it has the additional restriction that the type has to be a single basic numeric type. Also, PyTorch tensors are immutable, which means that we cannot change the values of each item in the array.</p>
<p>Unlike arrays, PyTorch tensors cannot be jagged. Also, they can live on the GPU, which is optimized for parallel computations. Therefore, given a large amount of data, it is much faster to use a GPU than a CPU. Additionally, PyTorch can automatically calculate derivatives.</p>
</section>
</section>
<section id="creating-arrays-and-tensors" class="level1">
<h1>Creating Arrays and Tensors</h1>
<p>Creating NumPy arrays and PyTorch tensors is very similar:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">data <span class="op" style="color: #5E5E5E;">=</span> [[<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">3</span>],[<span class="dv" style="color: #AD0000;">4</span>,<span class="dv" style="color: #AD0000;">5</span>,<span class="dv" style="color: #AD0000;">6</span>]]</span>
<span id="cb2-2">arr <span class="op" style="color: #5E5E5E;">=</span> array (data)</span>
<span id="cb2-3">tns <span class="op" style="color: #5E5E5E;">=</span> tensor(data)</span></code></pre></div>
</div>
<div class="cell" data-outputid="ad94e954-348e-4972-fd0c-fb02b7085f8c" data-execution_count="5">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">arr</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>array([[1, 2, 3],
       [4, 5, 6]])</code></pre>
</div>
</div>
<div class="cell" data-outputid="f45df576-31a1-4f1c-bbd7-b0dc907abb1b" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">tns</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>tensor([[1, 2, 3],
        [4, 5, 6]])</code></pre>
</div>
</div>
</section>
<section id="performing-operations" class="level1">
<h1>Performing Operations</h1>
<p>Operations on arrays and tensors mostly use the same syntax. For example,</p>
<div class="cell" data-outputid="5263e068-ab2f-41a8-d4b5-e0584658b5a8" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">tns<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span></span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>tensor([[2, 3, 4],
        [5, 6, 7]])</code></pre>
</div>
</div>
<div class="cell" data-outputid="a5cba05d-be80-4b36-aeee-138242c07ded" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">arr<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span></span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>array([[2, 3, 4],
       [5, 6, 7]])</code></pre>
</div>
</div>
<p>Refer to the <a href="https://pytorch.org/docs/stable/tensors.html">PyTorch documentation</a> for more operations. Happy learning!</p>


</section>

 ]]></description>
  <category>ml</category>
  <category>notes</category>
  <guid>https://blog.seongbin.me/posts/arrays-vs-tensors/index.html</guid>
  <pubDate>Sat, 30 Jul 2022 07:00:00 GMT</pubDate>
  <media:content url="https://blog.seongbin.me/posts/arrays-vs-tensors/numpy-vs-pytorch.jpeg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
